{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "sys.path.append(\".\")\n",
    "\n",
    "from datasetutil import gerar_estatiscas_df, substituir_valores_nulos , verify_gg4cc , save_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CSS = \"\"\"\n",
    ".output {\n",
    "    flex-direction: row;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "HTML('<style>{}</style>'.format(CSS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(zinput,nome_arquivo,\n",
    "         file_format=\".csv\",\n",
    "         separador=',',codificacao=\"iso-8859-1\",quote='True',skip_coments=False):\n",
    "    '''\n",
    "        Ler zip com os datasets\n",
    "    '''\n",
    "    zf = zipfile.ZipFile(zinput) \n",
    "\n",
    "    for file in zipfile.ZipFile.namelist(zf):\n",
    "        if file == nome_arquivo+file_format:\n",
    "            if quote == 'True':\n",
    "                df = pd.read_csv(zf.open(file), header=0, encoding=codificacao,\n",
    "                                 sep=separador,error_bad_lines=False)#,\n",
    "            else:\n",
    "                df = pd.read_csv(zf.open(file), header=0, encoding=codificacao,\n",
    "                                 sep=separador,error_bad_lines=False,\n",
    "                                 quotechar = quote)\n",
    "            if skip_coments:\n",
    "                df = pd.read_csv(zf.open(file), header=0, encoding=codificacao,\n",
    "                                 sep=separador,error_bad_lines=False,comment='#')\n",
    "                \n",
    "#             chunksize=640000000,\n",
    "#             nrows=15)\n",
    "            return df\n",
    "# df.iloc[:, [68,2,10,11,9,28,29,13,14,15,1,16]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procurar_duplicados(_df,a='id'):\n",
    "    counts = _df[a].value_counts()\n",
    "    return _df[_df[a].isin(counts.index[counts > 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retornar_ds_limpo(_df,chave='id'):\n",
    "    saida = []\n",
    "    \n",
    "    for k in _df[chave].unique():\n",
    "        if len(saida) == 0:\n",
    "            saida = _df[_df[chave] == k].head(1)\n",
    "        else:\n",
    "            saida = pd.concat([saida,_df[_df[chave] == k].head(1)])\n",
    "    return saida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magellan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## restaurantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### restaurantes4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_dir = 'D:'+os.sep+'Dados'+os.sep+'OneDrive'+os.sep+'Doutorado'+os.sep\n",
    "ds_files = base_dir + 'Datasets' + os.sep + 'Magellan Data Repository' + os.sep +'Magellan' + os.sep\n",
    "out_file = base_dir + 'workspace'+os.sep+'tl_pprl2'+os.sep+'datasets'+os.sep+'restaurants'+os.sep\n",
    "\n",
    "inputZip = 'yelp_yellowpages.zip'\n",
    "data = 'labeled_data'\n",
    "\n",
    "\n",
    "zip_file = 'processed_yelp-yellowpages.zip'\n",
    "\n",
    "df = read(ds_files+inputZip,data,skip_coments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gabarito\n",
    "gs = df[df.gold==1].iloc[:,[1,2]]\n",
    "\n",
    "\n",
    "colunas =  [ 'id','address','city','name','phone','state','zipcode' ]\n",
    "n_atibutos = len(colunas) -1\n",
    "\n",
    "inicio_atributos_ds1 = 3\n",
    "fim_atributos_ds1 = inicio_atributos_ds1 + n_atibutos\n",
    "\n",
    "\n",
    "atts = [1] + list(range(inicio_atributos_ds1,fim_atributos_ds1))\n",
    "ds1 = df.iloc[:,atts]\n",
    "\n",
    "atts = [2] + list(range(fim_atributos_ds1,fim_atributos_ds1+n_atibutos))\n",
    "ds2 = df.iloc[:,atts]\n",
    "\n",
    "ds1.columns = colunas\n",
    "ds2.columns = colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#nan em coluna de interiro\n",
    "ds2.zipcode = ds2.zipcode.fillna(-1)\n",
    "ds2.zipcode = ds2.zipcode.astype(int)\n",
    "ds2.zipcode = ds2.zipcode.astype(str)\n",
    "ds2.zipcode = ds2.zipcode.replace('-1', np.nan)\n",
    "\n",
    "ds1.zipcode = ds1.zipcode.fillna(-1)\n",
    "ds1.zipcode = ds1.zipcode.astype(int)\n",
    "ds1.zipcode = ds1.zipcode.astype(str)\n",
    "ds1.zipcode = ds1.zipcode.replace('-1', np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds1 = retornar_ds_limpo(ds1)\n",
    "ds2 = retornar_ds_limpo(ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Estatisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nan_list1 = ds1.isnull().sum()\n",
    "nan_list2 = ds2.isnull().sum()\n",
    "\n",
    "estatiscas1 = ds1.describe(include='object')\n",
    "estatiscas2 = ds2.describe(include='object')\n",
    "\n",
    "sdf1 = gerar_estatiscas_df(nan_list1,estatiscas1)\n",
    "sdf1['ds'] = ds1_name\n",
    "\n",
    "sdf2 = gerar_estatiscas_df(nan_list2,estatiscas2)\n",
    "sdf2['ds'] = ds2_name\n",
    "\n",
    "estatiscas = pd.concat([sdf1,sdf2])#.to_csv(ds_files+'stats_abt-buy.csv',sep=';')\n",
    "\n",
    "display(sdf1)\n",
    "display(sdf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Limpando os dados\n",
    "\n",
    "Sem valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gs.columns = ['id1','id2']\n",
    "gs.id1 = gs.id1.astype(str)\n",
    "gs.id2 = gs.id2.astype(str)\n",
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "remover_da_lista2 = []\n",
    "remover_da_lista1 = []\n",
    "\n",
    "for i in da.id1.unique():\n",
    "    remover_da_lista2.append(da[da.id1 == i].id2.values[-1])\n",
    "\n",
    "for i in db.id2.unique():\n",
    "    remover_da_lista1.append(db[db.id2 == i].id1.values[-1])\n",
    "\n",
    "print(remover_da_lista1)\n",
    "print(remover_da_lista2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cds1 = ds1[~ds1.id.isin(remover_da_lista1)]\n",
    "cds2 = ds2[~ds2.id.isin(remover_da_lista2)]\n",
    "\n",
    "gs = gs[~gs.id1.isin(remover_da_lista1)]\n",
    "gs = gs[~gs.id2.isin(remover_da_lista2)]\n",
    "\n",
    "ds1 = cds1\n",
    "ds2 = cds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Dataset clean-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_zip(ds1,ds2,gs,estatiscas,out_file + zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### restaurantes1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'D:'+os.sep+'Dados'+os.sep+'OneDrive'+os.sep+'Doutorado'+os.sep\n",
    "ds_files = base_dir + 'Datasets' + os.sep + 'Magellan Data Repository' + os.sep +'Magellan' + os.sep\n",
    "out_file = base_dir + 'workspace'+os.sep+'tl_pprl2'+os.sep+'datasets'+os.sep+'restaurants'+os.sep\n",
    "\n",
    "inputZip = 'yelp_zomato_big.zip'\n",
    "data = 'labeled_data'\n",
    "\n",
    "\n",
    "zip_file = 'processed_yelp-zomato.zip'\n",
    "\n",
    "df = read(ds_files+inputZip,data,skip_coments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gabarito\n",
    "gs = df[df.gold==1].iloc[:,[1,2]]\n",
    "\n",
    "colunas =  [ 'id','NAME','PHONENUMBER','ADDRESS' ]\n",
    "n_atibutos = len(colunas) -1\n",
    "\n",
    "inicio_atributos_ds1 = 3\n",
    "fim_atributos_ds1 = inicio_atributos_ds1 + n_atibutos\n",
    "\n",
    "\n",
    "atts = [1] + list(range(inicio_atributos_ds1,fim_atributos_ds1))\n",
    "ds1 = df.iloc[:,atts]\n",
    "\n",
    "atts = [2] + list(range(fim_atributos_ds1,fim_atributos_ds1+n_atibutos))\n",
    "ds2 = df.iloc[:,atts]\n",
    "\n",
    "ds1.columns = colunas\n",
    "ds2.columns = colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = retornar_ds_limpo(ds1)\n",
    "ds2 = retornar_ds_limpo(ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estatisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_list1 = ds1.isnull().sum()\n",
    "nan_list2 = ds2.isnull().sum()\n",
    "\n",
    "estatiscas1 = ds1.describe(include='object')\n",
    "estatiscas2 = ds2.describe(include='object')\n",
    "\n",
    "sdf1 = gerar_estatiscas_df(nan_list1,estatiscas1)\n",
    "sdf1['ds'] = ds1_name\n",
    "\n",
    "sdf2 = gerar_estatiscas_df(nan_list2,estatiscas2)\n",
    "sdf2['ds'] = ds2_name\n",
    "\n",
    "estatiscas = pd.concat([sdf1,sdf2])#.to_csv(ds_files+'stats_abt-buy.csv',sep=';')\n",
    "\n",
    "display(sdf1)\n",
    "display(sdf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpando os dados\n",
    "\n",
    "Sem valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.columns = ['id1','id2']\n",
    "gs.id1 = gs.id1.astype(str)\n",
    "gs.id2 = gs.id2.astype(str)\n",
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover_da_lista2 = []\n",
    "remover_da_lista1 = []\n",
    "\n",
    "for i in da.id1.unique():\n",
    "    remover_da_lista2.append(da[da.id1 == i].id2.values[-1])\n",
    "\n",
    "for i in db.id2.unique():\n",
    "    remover_da_lista1.append(db[db.id2 == i].id1.values[-1])\n",
    "\n",
    "print(remover_da_lista1)\n",
    "print(remover_da_lista2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cds1 = ds1[~ds1.id.isin(remover_da_lista1)]\n",
    "cds2 = ds2[~ds2.id.isin(remover_da_lista2)]\n",
    "\n",
    "gs = gs[~gs.id1.isin(remover_da_lista1)]\n",
    "gs = gs[~gs.id2.isin(remover_da_lista2)]\n",
    "\n",
    "ds1 = cds1\n",
    "ds2 = cds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset clean-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_zip(ds1,ds2,gs,estatiscas,out_file + zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Books1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_dir = 'D:'+os.sep+'Dados'+os.sep+'OneDrive'+os.sep+'Doutorado'+os.sep\n",
    "ds_files = base_dir + 'Datasets' + os.sep + 'Magellan Data Repository' + os.sep +'Magellan' + os.sep\n",
    "out_file = base_dir + 'workspace'+os.sep+'tl_pprl2'+os.sep+'datasets'+os.sep+'books'+os.sep\n",
    "\n",
    "inputZip = 'books1.zip'\n",
    "data = 'labeled_data'\n",
    "\n",
    "\n",
    "zip_file = 'processed_amazon-barnesnobel-small.zip'\n",
    "\n",
    "df = read(ds_files+inputZip,data,skip_coments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gabarito\n",
    "gs = df[df.gold_label==1].iloc[:,[1,2]]\n",
    "\n",
    "colunas =  [ 'id','title','pubyear','pages' ]\n",
    "n_atibutos = len(colunas) -1\n",
    "\n",
    "inicio_atributos_ds1 = 3\n",
    "fim_atributos_ds1 = inicio_atributos_ds1 + n_atibutos\n",
    "\n",
    "\n",
    "atts = [1] + list(range(inicio_atributos_ds1,fim_atributos_ds1))\n",
    "ds1 = df.iloc[:,atts]\n",
    "\n",
    "atts = [2] + list(range(fim_atributos_ds1,fim_atributos_ds1+n_atibutos))\n",
    "ds2 = df.iloc[:,atts]\n",
    "\n",
    "ds1.columns = colunas\n",
    "ds2.columns = colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds1.pages = ds1.pages.fillna(-1)\n",
    "ds1.pages = ds1.pages.astype(int)\n",
    "ds1.pages = ds1.pages.astype(str)\n",
    "ds1.pages = ds1.pages.replace('-1', np.nan)\n",
    "\n",
    "ds2.pages = ds2.pages.fillna(-1)\n",
    "ds2.pages = ds2.pages.astype(int)\n",
    "ds2.pages = ds2.pages.astype(str)\n",
    "ds2.pages = ds2.pages.replace('-1', np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds1 = retornar_ds_limpo(ds1)\n",
    "ds2 = retornar_ds_limpo(ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Estatisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nan_list1 = ds1.isnull().sum()\n",
    "nan_list2 = ds2.isnull().sum()\n",
    "\n",
    "estatiscas1 = ds1.describe(include='object')\n",
    "estatiscas2 = ds2.describe(include='object')\n",
    "\n",
    "sdf1 = gerar_estatiscas_df(nan_list1,estatiscas1)\n",
    "sdf1['ds'] = ds1_name\n",
    "\n",
    "sdf2 = gerar_estatiscas_df(nan_list2,estatiscas2)\n",
    "sdf2['ds'] = ds2_name\n",
    "\n",
    "estatiscas = pd.concat([sdf1,sdf2])#.to_csv(ds_files+'stats_abt-buy.csv',sep=';')\n",
    "\n",
    "display(sdf1)\n",
    "display(sdf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Limpando os dados\n",
    "\n",
    "Sem valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gs.columns = ['id1','id2']\n",
    "gs.id1 = gs.id1.astype(str)\n",
    "gs.id2 = gs.id2.astype(str)\n",
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Dataset clean-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_zip(ds1,ds2,gs,estatiscas,out_file + zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'D:'+os.sep+'Dados'+os.sep+'OneDrive'+os.sep+'Doutorado'+os.sep\n",
    "ds_files = base_dir + 'Datasets' + os.sep + 'Magellan Data Repository' + os.sep +'Magellan' + os.sep\n",
    "out_file = base_dir + 'workspace'+os.sep+'tl_pprl2'+os.sep+'datasets'+os.sep+'books'+os.sep\n",
    "\n",
    "inputZip = 'books2.zip'\n",
    "data = 'labeled_data'\n",
    "\n",
    "\n",
    "zip_file = 'processed_goodreads-barnesnobel.zip'\n",
    "\n",
    "df = read(ds_files+inputZip,data,skip_coments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gabarito\n",
    "gs = df[df.match_label==1].iloc[:,[1,2]]\n",
    "\n",
    "colunas_1 =  [ 'id', 'FirstAuthor', 'ISBN',\n",
    "       'ISBN13', 'PageCount', 'PublishDate',\n",
    "       'Publisher', 'SecondAuthor', 'ThirdAuthor',\n",
    "       'Title' ]\n",
    "\n",
    "colunas_2 =  [ 'id', 'FirstAuthor', 'SecondAuthor',\n",
    "       'ThirdAuthor', 'ISBN13', 'PageCount',\n",
    "       'PublishDate', 'Publisher', 'Title' ]\n",
    "\n",
    "n_atibutos = len(colunas_1) -1\n",
    "\n",
    "inicio_atributos_ds1 = 3\n",
    "fim_atributos_ds1 = inicio_atributos_ds1 + n_atibutos\n",
    "\n",
    "\n",
    "atts = [1] + list(range(inicio_atributos_ds1,fim_atributos_ds1))\n",
    "ds1 = df.iloc[:,atts]\n",
    "\n",
    "n_atibutos = len(colunas_2) -1\n",
    "atts = [2] + list(range(fim_atributos_ds1,fim_atributos_ds1+n_atibutos))\n",
    "ds2 = df.iloc[:,atts]\n",
    "\n",
    "ds1.columns = colunas_1\n",
    "ds2.columns = colunas_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = retornar_ds_limpo(ds1)\n",
    "ds2 = retornar_ds_limpo(ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estatisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_list1 = ds1.isnull().sum()\n",
    "nan_list2 = ds2.isnull().sum()\n",
    "\n",
    "estatiscas1 = ds1.describe(include='object')\n",
    "estatiscas2 = ds2.describe(include='object')\n",
    "\n",
    "sdf1 = gerar_estatiscas_df(nan_list1,estatiscas1)\n",
    "sdf1['ds'] = ds1_name\n",
    "\n",
    "sdf2 = gerar_estatiscas_df(nan_list2,estatiscas2)\n",
    "sdf2['ds'] = ds2_name\n",
    "\n",
    "estatiscas = pd.concat([sdf1,sdf2])#.to_csv(ds_files+'stats_abt-buy.csv',sep=';')\n",
    "\n",
    "display(sdf1)\n",
    "display(sdf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpando os dados\n",
    "\n",
    "Sem valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.columns = ['id1','id2']\n",
    "gs.id1 = gs.id1.astype(str)\n",
    "gs.id2 = gs.id2.astype(str)\n",
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover_da_lista2 = []\n",
    "remover_da_lista1 = []\n",
    "\n",
    "for i in da.id1.unique():\n",
    "    remover_da_lista2.append(da[da.id1 == i].id2.values[-1])\n",
    "\n",
    "for i in db.id2.unique():\n",
    "    remover_da_lista1.append(db[db.id2 == i].id1.values[-1])\n",
    "\n",
    "print(remover_da_lista1)\n",
    "print(remover_da_lista2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cds1 = ds1[~ds1.id.isin(remover_da_lista1)]\n",
    "cds2 = ds2[~ds2.id.isin(remover_da_lista2)]\n",
    "\n",
    "gs = gs[~gs.id1.isin(remover_da_lista1)]\n",
    "gs = gs[~gs.id2.isin(remover_da_lista2)]\n",
    "\n",
    "ds1 = cds1\n",
    "ds2 = cds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset clean-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "save_zip(ds1,ds2,gs,estatiscas,out_file + zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'D:'+os.sep+'Dados'+os.sep+'OneDrive'+os.sep+'Doutorado'+os.sep\n",
    "ds_files = base_dir + 'Datasets' + os.sep + 'Magellan Data Repository' + os.sep +'Magellan' + os.sep\n",
    "out_file = base_dir + 'workspace'+os.sep+'tl_pprl2'+os.sep+'datasets'+os.sep+'restaurants'+os.sep\n",
    "\n",
    "inputZip = 'books4.zip'\n",
    "data = 'labeled_data'\n",
    "\n",
    "\n",
    "zip_file = 'processed_amazon-barnesnobel.zip'\n",
    "\n",
    "df = read(ds_files+inputZip,data,skip_coments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gabarito\n",
    "gs = df[df.gold==1].iloc[:,[1,2]]\n",
    "\n",
    "colunas_1 =  [ 'id', 'Title', 'Author', 'ISBN_13', 'Publisher' ]\n",
    "\n",
    "colunas_2 =  colunas_1\n",
    "\n",
    "n_atibutos = len(colunas_1) -1\n",
    "\n",
    "inicio_atributos_ds1 = 3\n",
    "fim_atributos_ds1 = inicio_atributos_ds1 + n_atibutos\n",
    "\n",
    "\n",
    "atts = [1] + list(range(inicio_atributos_ds1,fim_atributos_ds1))\n",
    "ds1 = df.iloc[:,atts]\n",
    "\n",
    "n_atibutos = len(colunas_2) -1\n",
    "atts = [2] + list(range(fim_atributos_ds1,fim_atributos_ds1+n_atibutos))\n",
    "ds2 = df.iloc[:,atts]\n",
    "\n",
    "ds1.columns = colunas_1\n",
    "ds2.columns = colunas_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = retornar_ds_limpo(ds1)\n",
    "ds2 = retornar_ds_limpo(ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estatisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_list1 = ds1.isnull().sum()\n",
    "nan_list2 = ds2.isnull().sum()\n",
    "\n",
    "estatiscas1 = ds1.describe(include='object')\n",
    "estatiscas2 = ds2.describe(include='object')\n",
    "\n",
    "sdf1 = gerar_estatiscas_df(nan_list1,estatiscas1)\n",
    "sdf1['ds'] = ds1_name\n",
    "\n",
    "sdf2 = gerar_estatiscas_df(nan_list2,estatiscas2)\n",
    "sdf2['ds'] = ds2_name\n",
    "\n",
    "estatiscas = pd.concat([sdf1,sdf2])#.to_csv(ds_files+'stats_abt-buy.csv',sep=';')\n",
    "\n",
    "display(sdf1)\n",
    "display(sdf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpando os dados\n",
    "\n",
    "Sem valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.columns = ['id1','id2']\n",
    "gs.id1 = gs.id1.astype(str)\n",
    "gs.id2 = gs.id2.astype(str)\n",
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover_da_lista2 = []\n",
    "remover_da_lista1 = []\n",
    "\n",
    "for i in da.id1.unique():\n",
    "    remover_da_lista2.append(da[da.id1 == i].id2.values[-1])\n",
    "\n",
    "for i in db.id2.unique():\n",
    "    remover_da_lista1.append(db[db.id2 == i].id1.values[-1])\n",
    "\n",
    "print(remover_da_lista1)\n",
    "print(remover_da_lista2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cds1 = ds1[~ds1.id.isin(remover_da_lista1)]\n",
    "cds2 = ds2[~ds2.id.isin(remover_da_lista2)]\n",
    "\n",
    "gs = gs[~gs.id1.isin(remover_da_lista1)]\n",
    "gs = gs[~gs.id2.isin(remover_da_lista2)]\n",
    "\n",
    "ds1 = cds1\n",
    "ds2 = cds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset clean-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "save_zip(ds1,ds2,gs,estatiscas,out_file + zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'D:'+os.sep+'Dados'+os.sep+'OneDrive'+os.sep+'Doutorado'+os.sep\n",
    "ds_files = base_dir + 'Datasets' + os.sep + 'Magellan Data Repository' + os.sep +'Magellan' + os.sep\n",
    "out_file = base_dir + 'workspace'+os.sep+'tl_pprl2'+os.sep+'datasets'+os.sep+'movies'+os.sep\n",
    "\n",
    "inputZip = 'movies1.zip'\n",
    "data = 'labeled_data'\n",
    "\n",
    "\n",
    "zip_file = 'processed_imdb-rottentomatos.zip'\n",
    "\n",
    "df = read(ds_files+inputZip,data,skip_coments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gabarito\n",
    "gs = df[df.gold==1].iloc[:,[1,2]]\n",
    "\n",
    "colunas_1 =  [ 'id', 'Director', 'Name', 'Year' ]\n",
    "\n",
    "colunas_2 =  colunas_1\n",
    "\n",
    "n_atibutos = len(colunas_1) -1\n",
    "\n",
    "inicio_atributos_ds1 = 3\n",
    "fim_atributos_ds1 = inicio_atributos_ds1 + n_atibutos\n",
    "\n",
    "\n",
    "atts = [1] + list(range(inicio_atributos_ds1,fim_atributos_ds1))\n",
    "ds1 = df.iloc[:,atts]\n",
    "\n",
    "n_atibutos = len(colunas_2) -1\n",
    "atts = [2] + list(range(fim_atributos_ds1,fim_atributos_ds1+n_atibutos))\n",
    "ds2 = df.iloc[:,atts]\n",
    "\n",
    "ds1.columns = colunas_1\n",
    "ds2.columns = colunas_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1.Year = ds1.Year.astype(str)\n",
    "ds2.Year = ds2.Year.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = retornar_ds_limpo(ds1)\n",
    "ds2 = retornar_ds_limpo(ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estatisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_list1 = ds1.isnull().sum()\n",
    "nan_list2 = ds2.isnull().sum()\n",
    "\n",
    "estatiscas1 = ds1.describe(include='object')\n",
    "estatiscas2 = ds2.describe(include='object')\n",
    "\n",
    "sdf1 = gerar_estatiscas_df(nan_list1,estatiscas1)\n",
    "sdf1['ds'] = ds1_name\n",
    "\n",
    "sdf2 = gerar_estatiscas_df(nan_list2,estatiscas2)\n",
    "sdf2['ds'] = ds2_name\n",
    "\n",
    "estatiscas = pd.concat([sdf1,sdf2])#.to_csv(ds_files+'stats_abt-buy.csv',sep=';')\n",
    "\n",
    "display(sdf1)\n",
    "display(sdf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpando os dados\n",
    "\n",
    "Sem valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.columns = ['id1','id2']\n",
    "gs.id1 = gs.id1.astype(str)\n",
    "gs.id2 = gs.id2.astype(str)\n",
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover_da_lista2 = []\n",
    "remover_da_lista1 = []\n",
    "\n",
    "for i in da.id1.unique():\n",
    "    remover_da_lista2.append(da[da.id1 == i].id2.values[-1])\n",
    "\n",
    "for i in db.id2.unique():\n",
    "    remover_da_lista1.append(db[db.id2 == i].id1.values[-1])\n",
    "\n",
    "print(remover_da_lista1)\n",
    "print(remover_da_lista2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cds1 = ds1[~ds1.id.isin(remover_da_lista1)]\n",
    "cds2 = ds2[~ds2.id.isin(remover_da_lista2)]\n",
    "\n",
    "gs = gs[~gs.id1.isin(remover_da_lista1)]\n",
    "gs = gs[~gs.id2.isin(remover_da_lista2)]\n",
    "\n",
    "ds1 = cds1\n",
    "ds2 = cds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset clean-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "save_zip(ds1,ds2,gs,estatiscas,out_file + zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'D:'+os.sep+'Dados'+os.sep+'OneDrive'+os.sep+'Doutorado'+os.sep\n",
    "ds_files = base_dir + 'Datasets' + os.sep + 'Magellan Data Repository' + os.sep +'Magellan' + os.sep\n",
    "out_file = base_dir + 'workspace'+os.sep+'tl_pprl2'+os.sep+'datasets'+os.sep+'movies'+os.sep\n",
    "\n",
    "inputZip = 'movies2.zip'\n",
    "data = 'labeled_data'\n",
    "\n",
    "\n",
    "zip_file = 'processed_imdb-tmd.zip'\n",
    "\n",
    "df = read(ds_files+inputZip,data,skip_coments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gabarito\n",
    "gs = df[df.class_label==1].iloc[:,[1,2]]\n",
    "\n",
    "colunas_1 =  [ 'id', 'title', 'year' ]\n",
    "\n",
    "colunas_2 =  colunas_1\n",
    "\n",
    "n_atibutos = len(colunas_1) -1\n",
    "\n",
    "inicio_atributos_ds1 = 3\n",
    "fim_atributos_ds1 = inicio_atributos_ds1 + n_atibutos\n",
    "\n",
    "\n",
    "atts = [1] + list(range(inicio_atributos_ds1,fim_atributos_ds1))\n",
    "ds1 = df.iloc[:,atts]\n",
    "\n",
    "n_atibutos = len(colunas_2) -1\n",
    "atts = [2] + list(range(fim_atributos_ds1,fim_atributos_ds1+n_atibutos))\n",
    "ds2 = df.iloc[:,atts]\n",
    "\n",
    "ds1.columns = colunas_1\n",
    "ds2.columns = colunas_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.Year = ds2.Year.astype(str)\n",
    "ds2.Year = ds2.Year.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = retornar_ds_limpo(ds1)\n",
    "ds2 = retornar_ds_limpo(ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estatisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_list1 = ds1.isnull().sum()\n",
    "nan_list2 = ds2.isnull().sum()\n",
    "\n",
    "estatiscas1 = ds1.describe(include='object')\n",
    "estatiscas2 = ds2.describe(include='object')\n",
    "\n",
    "sdf1 = gerar_estatiscas_df(nan_list1,estatiscas1)\n",
    "sdf1['ds'] = ds1_name\n",
    "\n",
    "sdf2 = gerar_estatiscas_df(nan_list2,estatiscas2)\n",
    "sdf2['ds'] = ds2_name\n",
    "\n",
    "estatiscas = pd.concat([sdf1,sdf2])#.to_csv(ds_files+'stats_abt-buy.csv',sep=';')\n",
    "\n",
    "display(sdf1)\n",
    "display(sdf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpando os dados\n",
    "\n",
    "Sem valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.columns = ['id1','id2']\n",
    "gs.id1 = gs.id1.astype(str)\n",
    "gs.id2 = gs.id2.astype(str)\n",
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover_da_lista2 = []\n",
    "remover_da_lista1 = []\n",
    "\n",
    "for i in da.id1.unique():\n",
    "    remover_da_lista2.append(da[da.id1 == i].id2.values[-1])\n",
    "\n",
    "for i in db.id2.unique():\n",
    "    remover_da_lista1.append(db[db.id2 == i].id1.values[-1])\n",
    "\n",
    "print(remover_da_lista1)\n",
    "print(remover_da_lista2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cds1 = ds1[~ds1.id.isin(remover_da_lista1)]\n",
    "cds2 = ds2[~ds2.id.isin(remover_da_lista2)]\n",
    "\n",
    "gs = gs[~gs.id1.isin(remover_da_lista1)]\n",
    "gs = gs[~gs.id2.isin(remover_da_lista2)]\n",
    "\n",
    "ds1 = cds1\n",
    "ds2 = cds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset clean-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "save_zip(ds1,ds2,gs,estatiscas,out_file + zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'D:'+os.sep+'Dados'+os.sep+'OneDrive'+os.sep+'Doutorado'+os.sep\n",
    "ds_files = base_dir + 'Datasets' + os.sep + 'Magellan Data Repository' + os.sep +'Magellan' + os.sep\n",
    "out_file = base_dir + 'workspace'+os.sep+'tl_pprl2'+os.sep+'datasets'+os.sep+'movies'+os.sep\n",
    "\n",
    "inputZip = 'movies2.zip'\n",
    "data = 'labeled_data'\n",
    "\n",
    "\n",
    "zip_file = 'processed_imdb-tmd.zip'\n",
    "\n",
    "df = read(ds_files+inputZip,data,skip_coments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gabarito\n",
    "gs = df[df.class_label==1].iloc[:,[1,2]]\n",
    "\n",
    "colunas_1 =  [ 'id', 'title', 'year' ]\n",
    "\n",
    "colunas_2 =  colunas_1\n",
    "\n",
    "n_atibutos = len(colunas_1) -1\n",
    "\n",
    "inicio_atributos_ds1 = 3\n",
    "fim_atributos_ds1 = inicio_atributos_ds1 + n_atibutos\n",
    "\n",
    "\n",
    "atts = [1] + list(range(inicio_atributos_ds1,fim_atributos_ds1))\n",
    "ds1 = df.iloc[:,atts]\n",
    "\n",
    "n_atibutos = len(colunas_2) -1\n",
    "atts = [2] + list(range(fim_atributos_ds1,fim_atributos_ds1+n_atibutos))\n",
    "ds2 = df.iloc[:,atts]\n",
    "\n",
    "ds1.columns = colunas_1\n",
    "ds2.columns = colunas_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.Year = ds2.Year.astype(str)\n",
    "ds2.Year = ds2.Year.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = retornar_ds_limpo(ds1)\n",
    "ds2 = retornar_ds_limpo(ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estatisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_list1 = ds1.isnull().sum()\n",
    "nan_list2 = ds2.isnull().sum()\n",
    "\n",
    "estatiscas1 = ds1.describe(include='object')\n",
    "estatiscas2 = ds2.describe(include='object')\n",
    "\n",
    "sdf1 = gerar_estatiscas_df(nan_list1,estatiscas1)\n",
    "sdf1['ds'] = ds1_name\n",
    "\n",
    "sdf2 = gerar_estatiscas_df(nan_list2,estatiscas2)\n",
    "sdf2['ds'] = ds2_name\n",
    "\n",
    "estatiscas = pd.concat([sdf1,sdf2])#.to_csv(ds_files+'stats_abt-buy.csv',sep=';')\n",
    "\n",
    "display(sdf1)\n",
    "display(sdf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpando os dados\n",
    "\n",
    "Sem valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.columns = ['id1','id2']\n",
    "gs.id1 = gs.id1.astype(str)\n",
    "gs.id2 = gs.id2.astype(str)\n",
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover_da_lista2 = []\n",
    "remover_da_lista1 = []\n",
    "\n",
    "for i in da.id1.unique():\n",
    "    remover_da_lista2.append(da[da.id1 == i].id2.values[-1])\n",
    "\n",
    "for i in db.id2.unique():\n",
    "    remover_da_lista1.append(db[db.id2 == i].id1.values[-1])\n",
    "\n",
    "print(remover_da_lista1)\n",
    "print(remover_da_lista2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cds1 = ds1[~ds1.id.isin(remover_da_lista1)]\n",
    "cds2 = ds2[~ds2.id.isin(remover_da_lista2)]\n",
    "\n",
    "gs = gs[~gs.id1.isin(remover_da_lista1)]\n",
    "gs = gs[~gs.id2.isin(remover_da_lista2)]\n",
    "\n",
    "ds1 = cds1\n",
    "ds2 = cds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset clean-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "save_zip(ds1,ds2,gs,estatiscas,out_file + zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Corleone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## restaurants fodors-zagats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_dir = 'D:'+os.sep+'Dados'+os.sep+'OneDrive'+os.sep+'Doutorado'+os.sep\n",
    "ds_files = base_dir + 'Datasets' + os.sep + 'Magellan Data Repository' + os.sep +'Corleone' + os.sep\n",
    "out_file = base_dir + 'workspace'+os.sep+'tl_pprl2'+os.sep+'datasets'+os.sep+'restaurants'+os.sep\n",
    "\n",
    "inputZip = 'restaurants.zip'\n",
    "ds1_name = 'fodors'\n",
    "ds2_name = 'zagats'\n",
    "gs_name ='matches_fodors_zagats'\n",
    "\n",
    "\n",
    "zip_file = 'processed_fodors-zagats.zip'\n",
    "\n",
    "ds1 = read(ds_files+inputZip,ds1_name,quote=\"'\")\n",
    "ds2 = read(ds_files+inputZip,ds2_name,quote=\"'\")\n",
    "gs = read(ds_files+inputZip,gs_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Limpando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ds1.pop('title')\n",
    "# z = ds2.pop('title')\n",
    "# del z\n",
    "\n",
    "ds1.name = ds1.name.astype(str)\n",
    "ds1.addr = ds1.addr.astype(str)\n",
    "ds1.city = ds1.city.astype(str)\n",
    "ds1.phone = ds1.phone.astype(str)\n",
    "\n",
    "ds2.name = ds2.name.astype(str)\n",
    "ds2.addr = ds2.addr.astype(str)\n",
    "ds2.city = ds2.city.astype(str)\n",
    "ds2.phone = ds2.phone.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Estatisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nan_list1 = ds1.isnull().sum()\n",
    "nan_list2 = ds2.isnull().sum()\n",
    "\n",
    "estatiscas1 = ds1.describe(include='object')\n",
    "estatiscas2 = ds2.describe(include='object')\n",
    "\n",
    "sdf1 = gerar_estatiscas_df(nan_list1,estatiscas1)\n",
    "sdf1['ds'] = ds1_name\n",
    "\n",
    "sdf2 = gerar_estatiscas_df(nan_list2,estatiscas2)\n",
    "sdf2['ds'] = ds2_name\n",
    "\n",
    "estatiscas = pd.concat([sdf1,sdf2])#.to_csv(ds_files+'stats_abt-buy.csv',sep=';')\n",
    "\n",
    "display(sdf1)\n",
    "display(sdf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Limpando os dados\n",
    "\n",
    "Sem valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gs.columns = ['id1','id2']\n",
    "gs.id1 = gs.id1.astype(str)\n",
    "gs.id2 = gs.id2.astype(str)\n",
    "da, db = verify_gg4cc(gs)\n",
    "display(da.head(5))\n",
    "display(db.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Dataset clean-clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Salvando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_zip(ds1,ds2,gs,estatiscas,out_file + zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

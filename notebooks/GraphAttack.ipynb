{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thiagonobrega/tl-pprl/blob/master/notebooks/GraphAttack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmnMyRpbkq1_"
      },
      "source": [
        "## init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwbT3RgFkq2B"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import time\n",
        "import os\n",
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XkeRWGakq2C"
      },
      "source": [
        "### colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqdzMEHwkq2D"
      },
      "outputs": [],
      "source": [
        "!rm -rf tl-pprl/\n",
        "!git clone -b scale https://github.com/thiagonobrega/tl-pprl.git &> /dev/null\n",
        "# !pip install --upgrade gdown &> /dev/null\n",
        "!pip install -r tl-pprl/requirements.txt &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P0byZ3XtFA0"
      },
      "outputs": [],
      "source": [
        "sys.path.append(\"./tl-pprl/\")\n",
        "lpath = '.'+os.sep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqKTdeCjuWpT"
      },
      "source": [
        "### getting data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhu-npnOKxjz",
        "outputId": "c31a7afd-1f52-4b7a-c5f3-c09bee2a8abe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 45.1M  100 45.1M    0     0  28.0M      0  0:00:01  0:00:01 --:--:-- 28.0M\n"
          ]
        }
      ],
      "source": [
        "# https://drive.google.com/drive/folders/16sYLUUCaHy5WIb0jKJgcFxZr-MZ4XsLH?usp=sharing\n",
        "\n",
        "#books\n",
        "# url='https://drive.google.com/drive/folders/1W3dosBr5USCrAcn9aomf-tFvPByJOxvm' \n",
        "# !gdown --continue --folder --quiet -O priv_eval_tl_pprl/ {url}\n",
        "\n",
        "# url='https://drive.google.com/drive/folders/16sYLUUCaHy5WIb0jKJgcFxZr-MZ4XsLH'\n",
        "# #zip\n",
        "# url = 'https://drive.google.com/uc?id=16QmWGrKpN4o_95rbJ6I0Iy7bDZYj6RCx'\n",
        "# !gdown --continue --folder --quiet {url}\n",
        "#54mb\n",
        "# !gdown --continue --folder {url}\n",
        "\n",
        "# https://1drv.ms/u/s!AiduPqZxUF_qla8eaYkkqSAnIy--ig?e=keMfpi\n",
        "!curl 'https://1ccheq.bn.files.1drv.com/y4mVyu9oQbrpQLTZKEf-IzasRDsIZNUUijKD438tKW3YJiCOdT6D2Vb9oVCXTunKRaMBQDxOI8xYIxjbHu8SKo4B3LydiA-Y4f3D2LvNPz8hG4Q-teW9GPmGNs09Dz3-2W_VE7GYtmVEkizPMsx5n6ZDY22PYwWXdxXxploAnWm_fRgUMAhCWVbUMxfEeZish41SW7OMIOy5RqDBZajwQqaGw' \\\n",
        "  -H 'authority: 1ccheq.bn.files.1drv.com' \\\n",
        "  -H 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9' \\\n",
        "  -H 'accept-language: pt-BR,pt;q=0.9' \\\n",
        "  -H 'referer: https://onedrive.live.com/' \\\n",
        "  -H 'sec-fetch-dest: iframe' \\\n",
        "  -H 'sec-fetch-mode: navigate' \\\n",
        "  -H 'sec-fetch-site: cross-site' \\\n",
        "  -H 'sec-gpc: 1' \\\n",
        "  -H 'upgrade-insecure-requests: 1' \\\n",
        "  -H 'user-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36' \\\n",
        "  --compressed --output datasets.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZUS7S-YuZLK"
      },
      "source": [
        "### extracting data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df_FM4vpub_T"
      },
      "outputs": [],
      "source": [
        "!rm -rf datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp47LyCC_mTe",
        "outputId": "81679781-7f12-4905-a004-d76d07d5115d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  datasets.zip\n",
            "   creating: datasets/books/\n",
            "  inflating: datasets/books/processed_amazon-barnesnobel-small.zip  \n",
            "  inflating: datasets/books/processed_amazon-barnesnobel.zip  \n",
            "  inflating: datasets/books/processed_goodreads-barnesnobel.zip  \n",
            "   creating: datasets/census/\n",
            "  inflating: datasets/census/processed_census.zip  \n",
            "   creating: datasets/dblp-acm/\n",
            " extracting: datasets/dblp-acm/processed_DBLP-ACM.zip  \n",
            "   creating: datasets/movies/\n",
            "  inflating: datasets/movies/processed_imdb-rottentomatos.zip  \n",
            "  inflating: datasets/movies/processed_imdb-tmd.zip  \n",
            "   creating: datasets/mvr/\n",
            " extracting: datasets/mvr/michiganvoters_100000_0.1.zip  \n",
            " extracting: datasets/mvr/michiganvoters_10000_0.1.zip  \n",
            "  inflating: datasets/mvr/michiganvoters_2000_0.1.zip  \n",
            " extracting: datasets/mvr/michiganvoters_500000_0.1.zip  \n",
            "   creating: datasets/ncvr/\n",
            "   creating: datasets/restaurants/\n",
            "  inflating: datasets/restaurants/processed_fodors-zagats.zip  \n",
            "  inflating: datasets/restaurants/processed_yelp-yellowpages.zip  \n",
            "  inflating: datasets/restaurants/processed_yelp-zomato.zip  \n",
            "   creating: datasets/tse/\n",
            " extracting: datasets/tse/processed_tse-10000-8.zip  \n",
            "  inflating: datasets/tse/processed_tse-2000-8.zip  \n",
            " extracting: datasets/tse/processed_tse-25000-8.zip  \n",
            "  inflating: datasets/tse/processed_tse-5000-8.zip  \n",
            "   creating: datasets/yv-er/\n",
            " extracting: datasets/yv-er/processed_yver.zip  \n",
            " extracting: datasets/ncvr/priv_ncvoters_10000_0.1.zip  \n",
            " extracting: datasets/ncvr/priv_ncvoters_100000_0.1.zip  \n",
            " extracting: datasets/ncvr/priv_ncvoters_20000_0.1.zip  \n",
            " extracting: datasets/ncvr/priv_ncvoters_50000_0.1.zip  \n"
          ]
        }
      ],
      "source": [
        "!unzip datasets.zip -d datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMAJfRIFuiqi"
      },
      "source": [
        "### Selecting data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1mt7_EYFIpn"
      },
      "outputs": [],
      "source": [
        "!rm datasets/mvr/michiganvoters_500000_0.1.zip\n",
        "!rm datasets/mvr/michiganvoters_100000_0.1.zip\n",
        "# !rm datasets/mvr/michiganvoters_10000_0.1.zip\n",
        "!rm datasets/mvr/michiganvoters_2000_0.1.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmnJRHwlr4aI"
      },
      "outputs": [],
      "source": [
        "!rm -rf datasets/ncvr/priv_ncvoters_100000_0.1.zip\n",
        "!rm -rf datasets/ncvr/priv_ncvoters_50000_0.1.zip\n",
        "!rm -rf datasets/ncvr/priv_ncvoters_20000_0.1.zip  \n",
        "!rm -rf datasets/ncvr/priv_ncvoters_10000_0.1.zip  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z73Un4jNkq2F"
      },
      "outputs": [],
      "source": [
        "!mkdir saida\n",
        "!mkdir saida/graphs\n",
        "!mkdir saida/regre-models\n",
        "# !mv priv_eval_tl_pprl datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVDYuldWvEa7"
      },
      "source": [
        "### mount gdrive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B39AJP4oHMQt",
        "outputId": "93759129-3669-4756-e089-12e457c39a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2NIMqWTkq2F"
      },
      "source": [
        "### windows local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KduAQ8G3kq2F"
      },
      "outputs": [],
      "source": [
        "# lpath = 'd:'+os.sep+'Dados'+os.sep+'OneDrive'+os.sep+'Doutorado'+os.sep+'workspace'+os.sep+'tl@pprl'+os.sep\n",
        "# sys.path.append(lpath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMlnKapZwnTg"
      },
      "source": [
        "# run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uhymPLVkq2F"
      },
      "source": [
        "## setup geral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Snr4CoQntIOs"
      },
      "outputs": [],
      "source": [
        "from graph_attack.graph_attack import load_data_set, gen_q_gram_sets, genG, step04\n",
        "from graph_attack.utils import simcalc\n",
        "from graph_attack.utils.graph import SimGraph\n",
        "from graph_attack.graph_attack import encode_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0b5GRejvNOd"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF_fmYtv-D1R"
      },
      "outputs": [],
      "source": [
        "# plain_attr_list = [1,3,2] #census\n",
        "encode_base_data_set_name='ncvr'\n",
        "plain_attr_list= [2,3,1] #mvr\n",
        "plain_attr_list= [2,1]\n",
        "\n",
        "plain_base_data_set_name='ncvr'\n",
        "encode_attr_list = [2,3,1]\n",
        "encode_attr_list = [2,1]\n",
        "#MVR\n",
        "# plain_base_data_set_name='ncvr'\n",
        "plain_base_data_set_name='mvr'\n",
        "#Encoded file:    ['first_name', 'middle_name', 'last_name']\n",
        "encode_attr_list = [2,3,4]\n",
        "encode_attr_list = [2,4]\n",
        "\n",
        "# bf_enc_param = [0.3,0.3,0.4]\n",
        "bf_enc_param = [0.5,0.5]\n",
        "\n",
        "#similarity function\n",
        "plain_sim_funct_name='jacc'\n",
        "encode_sim_funct_name='jacc'\n",
        "\n",
        "\n",
        "# anonymization\n",
        "q=2\n",
        "padded_flag=False #sem pad\n",
        "# PAD_CHAR = chr(1) \n",
        "# bf\n",
        "encode_method='bf'\n",
        "bf_hash_type='rh'\n",
        "bf_num_hash_funct='opt'\n",
        "bf_len=200\n",
        "# bf_encode='clk' #rbf\n",
        "bf_encode='rbf'\n",
        "bf_harden='none'\n",
        "\n",
        "\n",
        "assert plain_sim_funct_name in ['dice','jacc','hamm']\n",
        "assert encode_sim_funct_name in ['dice','jacc','hamm']\n",
        "\n",
        "# Define encode and plaintext blocking methods\n",
        "#\n",
        "encode_blck_method = 'minhash' # hmlsh, minhash, soundex, none\n",
        "plain_blck_method  = 'minhash' # minhash, soundex, none\n",
        "# plain_blck_method = 'none'\n",
        "# plain_blck_method = 'minhash'\n",
        "# plain_blck_method = 'minhash'\n",
        "\n",
        "\n",
        "##\n",
        "# atack param\n",
        "sim_diff_adjust_flag=True\n",
        "min_sim = .05\n",
        "\n",
        "### para ajuste das similaridades\n",
        "if(sim_diff_adjust_flag == True):\n",
        "  # Define the regression model\n",
        "  if(encode_method == '2sh'):\n",
        "    regre_model_str = 'poly' # linear, poly, isotonic\n",
        "  else:\n",
        "    regre_model_str = 'linear' # linear, poly, isotonic\n",
        "else: \n",
        "  regre_model_str = 'none'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJJswONaKqAg"
      },
      "source": [
        "### similarity function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfGA0Kt6KsQQ"
      },
      "outputs": [],
      "source": [
        "# Initialise the actual similarity functions to be used\n",
        "#plain\n",
        "if (plain_sim_funct_name == 'dice'):\n",
        "    plain_sim_funct = simcalc.q_gram_dice_sim\n",
        "elif (plain_sim_funct_name == 'jacc'):\n",
        "  plain_sim_funct = simcalc.q_gram_jacc_sim\n",
        "else:\n",
        "  raise Exception('This should not happen: sim function not allowed')\n",
        "\n",
        "#encoded\n",
        "if(encode_method == '2sh'):\n",
        "  if (encode_sim_funct_name == 'dice'):\n",
        "    encode_sim_funct = simcalc.q_gram_dice_sim\n",
        "  elif (encode_sim_funct_name == 'jacc'):\n",
        "    encode_sim_funct = simcalc.q_gram_jacc_sim\n",
        "  else:\n",
        "    raise Exception('This should not happen: hamming is not allowed to 2sh') # nao faz sentido\n",
        "else:\n",
        "  if (encode_sim_funct_name == 'dice'):\n",
        "    encode_sim_funct = simcalc.bit_array_dice_sim\n",
        "  elif (encode_sim_funct_name == 'hamm'):\n",
        "    encode_sim_funct = simcalc.bit_array_hamm_sim\n",
        "  elif (encode_sim_funct_name == 'jacc'):\n",
        "    encode_sim_funct = simcalc.bit_array_jacc_sim\n",
        "  else:\n",
        "    raise Exception('This should not happen: sim function not defined!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcn3LOlyKkLw"
      },
      "source": [
        "### other variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFJY6lrbbk9D"
      },
      "outputs": [],
      "source": [
        "if (encode_method == 'bf'):\n",
        "  encode_method_str = 'bf-%s-%s-%d-%s-%s' % (bf_hash_type, bf_num_hash_funct,\n",
        "                      bf_len, bf_encode, bf_harden)\n",
        "elif (encode_method == 'tmh'):\n",
        "  encode_method_str = 'tmh-%d-%s-%d-%d-%d' % (tmh_num_hash_bits,\n",
        "                      tmh_hash_funct, tmh_num_tables, tmh_key_len, tmh_val_len)\n",
        "elif (encode_method == '2sh'):\n",
        "  encode_method_str = '2sh-%d-%d-%d' % (cmh_max_rand_int,\n",
        "                                        cmh_num_hash_funct, cmh_num_hash_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFWDlSwr7nwh"
      },
      "outputs": [],
      "source": [
        "today_str = time.strftime(\"%Y%m%d\", time.localtime())\n",
        "now_str =   time.strftime(\"%H%M\", time.localtime())\n",
        "today_time_str = time.strftime(\"%Y%m%d %H:%M:%S\", time.localtime())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkA21hdfZ8sh"
      },
      "outputs": [],
      "source": [
        "graph_path = 'graphs/'\n",
        "random_seed=101\n",
        "# bf_enc_param='clk'\n",
        "# bf_enc_param='rbf'\n",
        "\n",
        "generated_graph_flag = True  # Graphs were generated in this run\n",
        "graph_path = lpath + 'saida' + os.sep\n",
        "reg_path = lpath + 'saida' + os.sep\n",
        "\n",
        "encode_graph_file_name = graph_path + 'encode_graph_file_name'\n",
        "plain_graph_file_name  = graph_path + 'plain_graph_file_name'\n",
        "bf_num_hash_funct,bf_hash_type\n",
        "# ['linear','isotonic','poly',]\n",
        "regre_model_str='linear'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpySwIxGa6ha"
      },
      "source": [
        "### step 03 variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCAoDlcNa9Un"
      },
      "outputs": [],
      "source": [
        "regre_model_str='linear'\n",
        "\n",
        "# Get the overall minimum similarity for the graphs\n",
        "#\n",
        "min_sim = 1.0\n",
        "\n",
        "\n",
        "sim_diff_interval_size = 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5DyPPzMa99G"
      },
      "source": [
        "### Srep 04 variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NkagUPwbBu1"
      },
      "outputs": [],
      "source": [
        "#variaveis importantes\n",
        "# qg_graph_node_id_dict = {}\n",
        "# ba_graph_node_id_dict = {}\n",
        "\n",
        "# qg_graph_node_id_dict = {}\n",
        "# ba_graph_node_id_dict = {}\n",
        "\n",
        "# QG_sim_graph, BA_sim_graph, plain_graph_num_node, encode_graph_num_node, attck_res_header_list, attck_res_val_list\n",
        "\n",
        "### graph paramns\n",
        "\n",
        "# sim_hash_block_funct_list = ['all_in_one'] # 'all_in_one','hlsh'\n",
        "# sim_comp_funct_list = ['allpairs'] # 'allpairs', 'simtol'\n",
        "\n",
        "# graph_sim_list = [('[0.2]',[0.2])]\n",
        "graph_sim_list = [('[0.2]',[0.2])] #('[0.9-0.5]',[0.9,0.7,0.5]),\n",
        "                                     #('[0.5]',[0.5])\n",
        "\n",
        "graph_node_min_degr_list = [('5',5)]\n",
        "\n",
        "sim_hash_num_bit_list = [('1000',1000)]\n",
        "sim_hash_match_sim = 0.9\n",
        "sim_hash_block_funct_list = ['hlsh']\n",
        "sim_comp_funct_list = ['simtol']\n",
        "\n",
        "graph_feat_list_list = [('all',['node_freq','max_sim','min_sim','degree',\n",
        "                            'degree_histo1','degree_histo2','sim_avr',\n",
        "                            'sim_std','egonet_degree','egonet_density',\n",
        "                            #'pagerank', # very time consuming\n",
        "                            'between_central',\n",
        "                            #'closeness_central', # very time consuming\n",
        "                            'degree_central',\n",
        "                            #'eigenvec_central', # very time consuming\n",
        "                            ]),\n",
        "                            \n",
        "                     ('no-histo',['node_freq','max_sim','min_sim','degree',\n",
        "                                   'sim_avr', 'sim_std','egonet_degree',\n",
        "                                   'egonet_density','between_central',\n",
        "                                   'degree_central', 'eigenvec_central',\n",
        "                            ]),\n",
        "                     \n",
        "                     ('one-central',['node_freq','max_sim','min_sim','degree',\n",
        "                            'degree_histo1', 'degree_histo2', 'sim_avr',\n",
        "                            'sim_std','egonet_degree', 'egonet_density',\n",
        "                            'degree_central',\n",
        "                            ]),\n",
        "                     \n",
        "                     ('no-nf-central',['max_sim','min_sim','degree',\n",
        "                                          'degree_histo1','degree_histo2',\n",
        "                                          'sim_avr','sim_std',\n",
        "                                          'egonet_degree','egonet_density']),\n",
        "                     \n",
        "                     ('no-central',['node_freq','max_sim','min_sim',\n",
        "                                   'degree','degree_histo1',\n",
        "                                   'degree_histo2','sim_avr','sim_std',\n",
        "                                   'egonet_degree','egonet_density']),\n",
        "                     \n",
        "                     ('node2vec', ['node2vec']),\n",
        "]\n",
        "\n",
        "graph_feat_select_list = [('all','all'),\n",
        "                          #('non-zero','nonzero'),\n",
        "                          #'top-10':10,\n",
        "                          #('top-10',10)\n",
        "                          #('min-std-0.4',0.4),\n",
        "                         ]\n",
        "\n",
        "# For the 'simtol' comparison approach set the similarity tolerance values:\n",
        "# lower: how much lower the encoded similarity can be,\n",
        "# upper: how much higher the encoded similarity can be\n",
        "#\n",
        "bf_hash_sim_low_tol =  0.01  # If the encoding are Bloom filters\n",
        "bf_hash_sim_up_tol =   0.25\n",
        "\n",
        "tmh_hash_sim_low_tol =  0.05  # If the encoding are tabulation min hashes\n",
        "tmh_hash_sim_up_tol =   0.05\n",
        "\n",
        "cmh_hash_sim_low_tol =  0.05  # If the encoding are tabulation min hashes\n",
        "cmh_hash_sim_up_tol =   0.01\n",
        "\n",
        "adj_hash_sim_low_tol =  0.05  # If the encoded similarities have been adjusted\n",
        "adj_hash_sim_up_tol =   0.05\n",
        "\n",
        "#ajustes\n",
        "anon_hash_sim_low_tol = bf_hash_sim_low_tol\n",
        "anon_hash_sim_up_tol = bf_hash_sim_up_tol\n",
        "\n",
        "\n",
        "hash_sim_min_tol = 0.05  # Also for the 'simtol' comparison approach, how much\n",
        "                         # tolerance to count the number of edges\n",
        "\n",
        "# Numpy and Scipy Cosine give the same results and are always best - so only\n",
        "# use those\n",
        "#\n",
        "#final_sim_funct_list = ['lsh_cosine','cosine_scipy','cosine_numpy',\n",
        "#                        'euclidean']\n",
        "#final_sim_funct_list = ['edge_sim_diff']\n",
        "\n",
        "# The parameters for the Hamming LSH blocking of the similarity hashes in the\n",
        "# graph matching step\n",
        "#\n",
        "# if('ncvoter' in encode_data_set_name or 'ncvr' in encode_data_set_name):\n",
        "graph_block_hlsh_num_sample =      20\n",
        "graph_block_hlsh_rel_sample_size = 10  # Divide bit array length by this\n",
        "\n",
        "# else:\n",
        "#   graph_block_hlsh_num_sample =      50\n",
        "#   graph_block_hlsh_rel_sample_size = 50  # Divide bit array length by this\n",
        "\n",
        "# Also get a sorted list of all similarities over all parameter settings\n",
        "#\n",
        "all_sim_set = set()\n",
        "\n",
        "for (sim_list_name, sim_list) in graph_sim_list:\n",
        "  min_sim = min(min_sim, min(sim_list))\n",
        "  all_sim_set = all_sim_set | set(sim_list)\n",
        "\n",
        "all_sim_list = sorted(all_sim_set)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nr4kNko7hrh"
      },
      "source": [
        "## Exec: Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgfiGlrUU02R"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "\n",
        "def extract_ds(ed,filename,target='a.csv'):\n",
        "  with ZipFile(ed+filename, 'r') as zipObj:\n",
        "    listOfFileNames = zipObj.namelist()\n",
        "    for fileName in listOfFileNames:\n",
        "        if fileName == target:\n",
        "            zipObj.extract(fileName, ed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA3UCd4BS0Rk"
      },
      "outputs": [],
      "source": [
        "encoded_dir = 'datasets'+os.sep+encode_base_data_set_name+os.sep\n",
        "plain_dir = 'datasets'+os.sep+plain_base_data_set_name+os.sep\n",
        "\n",
        "enc_files = [f for f in os.listdir(encoded_dir) if '0.1.zip' in f]\n",
        "plain_files = [f for f in os.listdir(plain_dir) if '.zip' in f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk-t3NeIvLHs"
      },
      "outputs": [],
      "source": [
        "# formatter = logging.Formatter('%(asctime)s %(levelname)s: %(funcName)s:%(lineno)d %(message)s')\n",
        "formatter = logging.Formatter('%(funcName)s:%(lineno)d %(message)s')\n",
        "\n",
        "# create logger with 'spam_application'\n",
        "logger = logging.getLogger()\n",
        "logger.handlers.pop(0)\n",
        "# logger.handlers\n",
        "\n",
        "logger.setLevel(logging.DEBUG)\n",
        "fh = logging.FileHandler('exec_100k.log')\n",
        "fh.setLevel(logging.DEBUG)\n",
        "fh.setFormatter(formatter)\n",
        "\n",
        "# just so we can see things as they happen in stdout/stderr\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.ERROR)\n",
        "ch.setFormatter(formatter)\n",
        "\n",
        "# add the handlers to the logger\n",
        "logger.addHandler(fh)\n",
        "logger.addHandler(ch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyYNv9I1mWpA"
      },
      "source": [
        "### run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54bL56R-QRtz",
        "outputId": "399b4895-490c-4b00-d6b5-ec1e4895f760"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "michiganvoters_10000_0.1.zip priv_ncvoters_100000_0.1.zip\n",
            "\n",
            "*** Warning: Different attributes used in encoded and plain-text files:\n",
            "***   Encoded file:    ['first_name', 'last_name']\n",
            "***   Plain-text file: ['firstname', 'lastname']\n",
            "The two datasets have 0 overlapping records (0.0 %)\n",
            "**** Step 2 :: Encoding Dataset\n",
            "**** Step 2 :: Done\n",
            "**** Step 3 :: Generating Graph\n",
            "**** Step 3 :: Done\n",
            "**** Step 4 :: Starting...\n"
          ]
        }
      ],
      "source": [
        "#itera nos dados\n",
        "# for plain_file in plain_files[:1]:\n",
        "for plain_file in plain_files:\n",
        "  #extrai os dados\n",
        "  extract_ds(plain_dir,plain_file)\n",
        "  # le os dados planos\n",
        "  plain_rec_attr_val_dict, plain_attr_name_list, plain_num_rec_loaded, plain_soundex_val_dict =\\\n",
        "      load_data_set(lpath + plain_dir+'a.csv',plain_attr_list,0,[2], #TODO:jogar esses parametros p cima\n",
        "                      header_line_flag=True,\n",
        "                      col_sep_char=';')\n",
        "  #removendo os arquivos apos o experimento\n",
        "  os.remove(plain_dir+'a.csv')\n",
        "\n",
        "  # for enc_file in enc_files[:1]:\n",
        "  for enc_file in enc_files:\n",
        "    #extrai os dados\n",
        "    print('')\n",
        "    print(plain_file,enc_file)\n",
        "    print('')\n",
        "    extract_ds(encoded_dir,enc_file,target='b.csv')\n",
        "    #le os dados codificados\n",
        "    encode_rec_attr_val_dict, encode_attr_name_list,encode_num_rec_loaded, encode_soundex_val_dict =\\\n",
        "        load_data_set(lpath +encoded_dir+'b.csv',encode_attr_list,0,[2],#TODO:jogar esses parametros p cima\n",
        "                        header_line_flag=True,\n",
        "                        col_sep_char=';')\n",
        "    # encode_rec_attr_val_dict, encode_attr_name_list,encode_num_rec_loaded, encode_soundex_val_dict =\\\n",
        "    # load_data_set(lpath + plain_dir+'b.csv',plain_attr_list,0,[2], #TODO:jogar esses parametros p cima\n",
        "    #                   header_line_flag=True,\n",
        "    #                   col_sep_char=';')\n",
        "\n",
        "    #removendo os arquivos apos o experimento\n",
        "    # os.remove(encoded_dir+'a.csv')\n",
        "\n",
        "    if (encode_attr_name_list != plain_attr_name_list):\n",
        "      print('*** Warning: Different attributes used in encoded and ' + \\\n",
        "            'plain-text files:')\n",
        "      print('***   Encoded file:   ', encode_attr_name_list)\n",
        "      print('***   Plain-text file:', plain_attr_name_list)\n",
        "      # encode_attr_name_list = plain_attr_name_list\n",
        "\n",
        "    # Generate q-gram sets for records\n",
        "    #\n",
        "    encode_q_gram_dict = gen_q_gram_sets(encode_rec_attr_val_dict, q, padded_flag)\n",
        "    plain_q_gram_dict =  gen_q_gram_sets(plain_rec_attr_val_dict, q, padded_flag)\n",
        "\n",
        "    enc_id_set = set(encode_q_gram_dict.keys())\n",
        "    plain_id_set = set(plain_q_gram_dict.keys())\n",
        "\n",
        "    common_rec_id_set = enc_id_set.intersection(plain_id_set)\n",
        "\n",
        "    print('The two datasets have %d overlapping records (%.1f %%)' % \\\n",
        "          (len(common_rec_id_set), 200*float(len(common_rec_id_set))/(len(enc_id_set)+len(plain_id_set)))\n",
        "    )\n",
        "\n",
        "    ###########\n",
        "    ########### Step 02\n",
        "    ###########\n",
        "    \n",
        "    # Create the graph pickle file names, and if they both are available then\n",
        "    # load the graphs from files\n",
        "    #\n",
        "    plain_attr_list_str =  str(plain_attr_list).replace(', ','_')[1:-1]\n",
        "    encode_attr_list_str = str(encode_attr_list).replace(', ','_')[1:-1]\n",
        "\n",
        "    # The generated graph file names\n",
        "    #          \n",
        "    encode_str = 'encode-sim-graph-%s-%s-%d-%d-%s-%s-%s-%s-%.3f-%s' \\\n",
        "                % (encode_base_data_set_name, encode_attr_list_str,\n",
        "                    encode_num_rec_loaded, q, str(padded_flag).lower(),\n",
        "                    regre_model_str.lower(),\n",
        "                    encode_sim_funct_name, encode_blck_method, min_sim,\n",
        "                    encode_method_str)\n",
        "                \n",
        "    plain_str = 'plain-sim-graph-%s-%s-%d-%d-%s-%s-%s-%s-%.3f-%s' % \\\n",
        "                (plain_base_data_set_name, plain_attr_list_str,\n",
        "                  plain_num_rec_loaded, q, str(padded_flag).lower(),\n",
        "                  regre_model_str.lower(),\n",
        "                  plain_sim_funct_name, plain_blck_method, min_sim,\n",
        "                  encode_method_str)\n",
        "\n",
        "    encode_graph_file_name = encode_str + '.pickle'\n",
        "    plain_graph_file_name = plain_str + '.pickle'\n",
        "\n",
        "\n",
        "    encode_graph_file_name = graph_path + encode_graph_file_name\n",
        "    plain_graph_file_name  = graph_path + plain_graph_file_name\n",
        "\n",
        "    ## encode dataset\n",
        "    print('**** Step 2 :: Encoding Dataset')\n",
        "    e = encode_ds(encode_q_gram_dict,encode_attr_list,encode_rec_attr_val_dict,\n",
        "                plain_rec_attr_val_dict,\n",
        "                encode_method,bf_len,q,bf_encode,\n",
        "                bf_hash_type=bf_hash_type,\n",
        "                bf_enc_param=bf_enc_param,\n",
        "                bf_num_hash_funct=bf_num_hash_funct,\n",
        "                random_seed=random_seed, # definir\n",
        "            )\n",
        "    \n",
        "    encode_hash_dict,hashing_time,plain_num_ent,encode_num_ent,common_num_ent = e[0],e[1],e[2],e[3],e[4]\n",
        "    print('**** Step 2 :: Done')\n",
        "    \n",
        "    \n",
        "    ###############\n",
        "    ############### STEP 03\n",
        "    ###############\n",
        "    print('**** Step 3 :: Generating Graph')\n",
        "    try:\n",
        "      z = genG(plain_num_ent,encode_num_ent,\n",
        "          encode_method, # encoding\n",
        "          # encode\n",
        "          encode_sim_funct_name,\n",
        "          encode_hash_dict,\n",
        "          encode_q_gram_dict,\n",
        "          encode_rec_attr_val_dict,\n",
        "          encode_blck_method, #'hmlsh': Hamming LSH blocking 'minhash' :  Min-hash LSH blocking\n",
        "          #plain\n",
        "          plain_q_gram_dict,\n",
        "          plain_rec_attr_val_dict,\n",
        "          plain_sim_funct_name,\n",
        "          plain_blck_method,\n",
        "          #soundex\n",
        "          encode_soundex_val_dict,\n",
        "          plain_soundex_val_dict,\n",
        "          # encoded_plain_vars\n",
        "          # regression\n",
        "          regre_model_str,\n",
        "          plain_base_data_set_name,\n",
        "          plain_attr_list_str,\n",
        "          q,\n",
        "          padded_flag,\n",
        "          encode_method_str,\n",
        "          encode_base_data_set_name,plain_num_rec_loaded,encode_attr_list_str,encode_num_rec_loaded,\n",
        "          #outros\n",
        "          min_sim, # vem la de cima\n",
        "          all_sim_list,\n",
        "          #\n",
        "          num_samples = 20000, # regression num samples\n",
        "          random_seed=101,\n",
        "          ## anonimizacao\n",
        "          bf_hash_type=bf_hash_type,\n",
        "          bf_num_hash_funct=bf_num_hash_funct,\n",
        "          bf_len=bf_len,\n",
        "          bf_encode=bf_encode,\n",
        "          bf_harden='None',\n",
        "          # leitura dos grafos\n",
        "          graph_path = graph_path,\n",
        "          regre_file_path = reg_path,\n",
        "          plain_graph_file_name=plain_graph_file_name,\n",
        "          encode_graph_file_name=encode_graph_file_name,\n",
        "          # utilizar apenas atributos em comum\n",
        "          include_only_common = False,\n",
        "          common_rec_id_set={},#setar caso seja utilizado\n",
        "          #nao sei o que e isso mas estava hardoced\n",
        "          same_ba_blck = False,\n",
        "          #ajueste de similaridade\n",
        "          sim_diff_interval_size = sim_diff_interval_size,\n",
        "          sim_diff_adjust_flag=sim_diff_adjust_flag\n",
        "        ) \n",
        "      \n",
        "      # repassando variaveis\n",
        "      QG_sim_graph, BA_sim_graph, plain_graph_num_node, encode_graph_num_node,\\\n",
        "      attck_res_header_list, attck_res_val_list = z\n",
        "      print('**** Step 3 :: Done')\n",
        "\n",
        "      #########\n",
        "      ######### Step 04 and 05\n",
        "      #########\n",
        "\n",
        "      exp_params = [ plain_attr_list_str,plain_num_rec_loaded,encode_attr_list_str,\\\n",
        "        encode_num_rec_loaded,padded_flag,q,plain_sim_funct_name,\\\n",
        "        encode_sim_funct_name,encode_method_str,encode_blck_method,\\\n",
        "        plain_blck_method\n",
        "      ]\n",
        "      \n",
        "      p = plain_file.split('.')[0]\n",
        "      e = enc_file.split('.')[0]\n",
        "      ataque_file = 'ataque-'+p+'-'+e+'.csv'\n",
        "\n",
        "      print('**** Step 4 :: Starting...')\n",
        "\n",
        "      step04(QG_sim_graph,BA_sim_graph,\n",
        "          graph_node_min_degr_list,\n",
        "          min_sim,\n",
        "          all_sim_list,\n",
        "          plain_rec_attr_val_dict,\n",
        "          encode_rec_attr_val_dict,\n",
        "          plain_q_gram_dict,\n",
        "          encode_q_gram_dict,\n",
        "          #\n",
        "          sim_comp_funct_list,\n",
        "          sim_hash_num_bit_list,\n",
        "          sim_hash_block_funct_list,\n",
        "          # sim_hash_match_sim,\n",
        "          #\n",
        "          hash_sim_min_tol,\n",
        "          adj_hash_sim_low_tol,\n",
        "          adj_hash_sim_up_tol,\n",
        "          anon_hash_sim_low_tol,\n",
        "          anon_hash_sim_up_tol,\n",
        "          #\n",
        "          plain_str,\n",
        "          encode_str,\n",
        "          #\n",
        "          graph_sim_list,\n",
        "          graph_feat_list_list,\n",
        "          graph_feat_select_list,\n",
        "          #\n",
        "          plain_base_data_set_name,\n",
        "          encode_base_data_set_name,\n",
        "          #\n",
        "          encode_method, \n",
        "          #\n",
        "          attck_res_header_list, attck_res_val_list,\n",
        "          #\n",
        "          sim_hash_match_sim=sim_hash_match_sim,\n",
        "          # blocagem\n",
        "          graph_block_hlsh_rel_sample_size=graph_block_hlsh_rel_sample_size,\n",
        "          graph_block_hlsh_num_sample=graph_block_hlsh_num_sample,\n",
        "          #\n",
        "          plain_graph_num_node=0,\n",
        "          encode_graph_num_node=0,\n",
        "          # attack_res_file_name='ataque.csv',\n",
        "          attack_res_file_name=ataque_file,\n",
        "          random_seed=random_seed,\n",
        "          feat_path = 'saida' + os.sep + 'feats' + os.sep,\n",
        "          sim_diff_adjust_flag=sim_diff_adjust_flag,\n",
        "          exp_params=exp_params,\n",
        "      ) #fim do step 04\n",
        "      # copiar aqui\n",
        "      print('**** Step 4 :: Done')\n",
        "      !cp *.csv '/content/drive/MyDrive/Colab Notebooks/'\n",
        "    # except ValueError:\n",
        "    except KeyError:\n",
        "      # ValueError: zero-size array to reduction operation minimum which has no identity\n",
        "      # investigar esse erro\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx-md_31m8LM"
      },
      "outputs": [],
      "source": [
        "!cp *.log '/content/drive/MyDrive/Colab Notebooks/'\n",
        "!cp *.csv '/content/drive/MyDrive/Colab Notebooks/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H3HRRwHWIso"
      },
      "source": [
        "ab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbURI_jLR9aW"
      },
      "source": [
        "## nao rodar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0duUVUwdp5eH",
        "outputId": "a914fa56-6321-47c2-aac3-c9e85adc979a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cw1297608\n",
            "bitarray('111000110010110110000000100000000100100101000001000010100100100011101011010001000010001011001100010100000010000011100000')\n",
            "('ei', 'en', 'er', 'fe', 'gh', 'if', 'ig', 'je', 'le', 'ni', 'nn')\n",
            "('ei', 'en', 'er', 'fe', 'gh', 'if', 'ig', 'je', 'le', 'ni')\n"
          ]
        }
      ],
      "source": [
        "print(ent_id)\n",
        "print(node_key_val)\n",
        "\n",
        "print(encode_key_q_gram_key_dict[node_key_val])\n",
        "print(tuple(sorted(q_gram_set)))\n",
        "\n",
        "encode_q_gram_dict['cw1297608']\n",
        "'bf38941'\n",
        "lflag = node_key_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "cMVDtuPFpLMe",
        "outputId": "44b7eff9-854f-40d2-8cf6-82da89891a08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bf38941\n",
            "1\n",
            "cw1297608\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-0fe995a9e1bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_key_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencode_key_q_gram_key_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mencode_key_q_gram_key_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_key_val\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_gram_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mencode_key_q_gram_key_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_key_val\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_gram_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# First add all records as nodes to the two graphs, and also generate two\n",
        "# dictionaries where keys are the values added as nodes into the graphs while\n",
        "# values are the original sets or encodings (that cannot be used as nodes)\n",
        "# This ensures we compare each unique pair for q-grams / bit-arrays only once\n",
        "#\n",
        "plain_node_val_dict =  {}\n",
        "encode_node_val_dict = {}\n",
        "\n",
        "# A dictionary where keys are node key values based on q-grams (i.e.\n",
        "# plain-text values) and values are their corresponding bit arrays (these\n",
        "# are the true matching q-gram sets to bit arrays to be used for the\n",
        "# similarity difference calculations)\n",
        "#\n",
        "encode_plain_node_val_dict = {}\n",
        "encode_key_q_gram_key_dict = {}\n",
        "\n",
        "for (ent_id, bit_array) in encode_hash_dict.items():\n",
        "  if ent_id == 'cw1297608':\n",
        "    print(1)\n",
        "  q_gram_set = encode_q_gram_dict[ent_id]\n",
        "\n",
        "  if(encode_method == '2sh'):\n",
        "    node_key_val = [str(val) for val in bit_array]\n",
        "    node_key_val = tuple(sorted(node_key_val))  # Sets cannot be dictionary keys\n",
        "  else:\n",
        "    node_key_val = str(bit_array)  # Bit arrays cannot be dictionary keys  \n",
        "\n",
        "  if lflag == node_key_val:\n",
        "    print(ent_id)\n",
        "\n",
        "  encode_node_val_dict[node_key_val] = (q_gram_set, bit_array)\n",
        "\n",
        "  if(node_key_val in encode_key_q_gram_key_dict):\n",
        "    assert encode_key_q_gram_key_dict[node_key_val] == tuple(sorted(q_gram_set))\n",
        "  else:\n",
        "    encode_key_q_gram_key_dict[node_key_val] = tuple(sorted(q_gram_set))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFxuHj6jl6E2"
      },
      "source": [
        "## Clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8UVQuRer33U"
      },
      "outputs": [],
      "source": [
        "!rm *.csv\n",
        "!rm *.log\n",
        "!rm *.png\n",
        "!rm saida/*.pickle\n",
        "!rm saida/*.sav\n",
        "!rm saida/feats/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaAZzJnUMeLj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cqKTdeCjuWpT",
        "xZUS7S-YuZLK",
        "rMAJfRIFuiqi",
        "KVDYuldWvEa7",
        "D2NIMqWTkq2F",
        "4uhymPLVkq2F",
        "q0b5GRejvNOd",
        "eJJswONaKqAg",
        "bcn3LOlyKkLw",
        "DpySwIxGa6ha",
        "1nr4kNko7hrh",
        "LbURI_jLR9aW",
        "QFxuHj6jl6E2"
      ],
      "machine_shape": "hm",
      "name": "GraphAttack2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "979abc0a35086451dcf41b2c584779315bebbe9a84e446060e411448c894138c"
    },
    "kernelspec": {
      "display_name": "tl_pprl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
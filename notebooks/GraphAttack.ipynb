{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmnMyRpbkq1_"
      },
      "source": [
        "## init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dwbT3RgFkq2B"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XkeRWGakq2C"
      },
      "source": [
        "### colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gqdzMEHwkq2D"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'rm' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n",
            "A sintaxe do comando est� incorreta.\n",
            "A sintaxe do comando est� incorreta.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf tl-pprl/\n",
        "!git clone -b attack https://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@github.com/thiagonobrega/tl-pprl.git &> /dev/null\n",
        "!pip install -r tl-pprl/requirements.txt &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9P0byZ3XtFA0"
      },
      "outputs": [],
      "source": [
        "sys.path.append(\"./tl-pprl/\")\n",
        "lpath = '.'+os.sep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BGiFMZHGkq2E"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A sintaxe do comando est� incorreta.\n",
            "A sintaxe do comando est� incorreta.\n",
            "A sintaxe do comando est� incorreta.\n",
            "A sintaxe do comando est� incorreta.\n",
            "A sintaxe do comando est� incorreta.\n",
            "A sintaxe do comando est� incorreta.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p datasets/ncvr\n",
        "!gdown --id 17FAGLWIkpbfpZ1hgFhu1nJzCzVGGJzs7 --output datasets/ncvr/ncvr.zip &>/dev/null\n",
        "!unzip -o datasets/ncvr/ncvr.zip -d datasets/ncvr/ &>/dev/null\n",
        "\n",
        "!mkdir -p datasets/census\n",
        "!gdown --id 1sTCEbghL8xRPkO8jy8dCjIij99cFv8T2 --output datasets/census/census.zip &>/dev/null\n",
        "!unzip -o datasets/census/census.zip -d datasets/census/ &>/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Z73Un4jNkq2F"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "J� existe uma subpasta ou um arquivo graphs.\n",
            "J� existe uma subpasta ou um arquivo regre-models.\n",
            "J� existe uma subpasta ou um arquivo saida.\n"
          ]
        }
      ],
      "source": [
        "!mkdir graphs\n",
        "!mkdir regre-models\n",
        "!mkdir saida"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2NIMqWTkq2F"
      },
      "source": [
        "### windows local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KduAQ8G3kq2F"
      },
      "outputs": [],
      "source": [
        "lpath = 'd:'+os.sep+'Dados'+os.sep+'OneDrive'+os.sep+'Doutorado'+os.sep+'workspace'+os.sep+'tl@pprl'+os.sep\n",
        "sys.path.append(lpath)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uhymPLVkq2F"
      },
      "source": [
        "## geral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Snr4CoQntIOs"
      },
      "outputs": [],
      "source": [
        "from graph_attack.graph_attack import load_data_set, gen_q_gram_sets, genG\n",
        "from graph_attack.utils import simcalc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SF_fmYtv-D1R"
      },
      "outputs": [],
      "source": [
        "#\n",
        "encode_base_data_set_name='ncvr'\n",
        "plain_base_data_set_name='census'\n",
        "\n",
        "#similarity function\n",
        "plain_sim_funct_name='jacc'\n",
        "encode_sim_funct_name='jacc'\n",
        "\n",
        "plain_attr_list = [1,3,2]\n",
        "encode_attr_list = [2,3,4]\n",
        "\n",
        "# anonymization\n",
        "q=2\n",
        "padded_flag=False #sem pad\n",
        "# PAD_CHAR = chr(1) \n",
        "# bf\n",
        "encode_method='bf'\n",
        "bf_hash_type='rh'\n",
        "bf_num_hash_funct='opt'\n",
        "bf_len=200\n",
        "bf_encode='clk' #rbf\n",
        "bf_harden='none'\n",
        "\n",
        "\n",
        "assert plain_sim_funct_name in ['dice','jacc','hamm']\n",
        "assert encode_sim_funct_name in ['dice','jacc','hamm']\n",
        "\n",
        "# Define encode and plaintext blocking methods\n",
        "#\n",
        "encode_blck_method = 'minhash' # hmlsh, minhash, soundex, none\n",
        "plain_blck_method  = 'minhash' # minhash, soundex, none\n",
        "\n",
        "\n",
        "##\n",
        "# atack param\n",
        "sim_diff_adjust_flag=False\n",
        "min_sim = .05\n",
        "\n",
        "### para ajuste das similaridades\n",
        "if(sim_diff_adjust_flag == True):\n",
        "  # Define the regression model\n",
        "  if(encode_method == '2sh'):\n",
        "    regre_model_str = 'poly' # linear, poly, isotonic\n",
        "  else:\n",
        "    regre_model_str = 'linear' # linear, poly, isotonic\n",
        "else: \n",
        "  regre_model_str = 'none'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJJswONaKqAg"
      },
      "source": [
        "#### similarity function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tfGA0Kt6KsQQ"
      },
      "outputs": [],
      "source": [
        "# Initialise the actual similarity functions to be used\n",
        "#plain\n",
        "if (plain_sim_funct_name == 'dice'):\n",
        "    plain_sim_funct = simcalc.q_gram_dice_sim\n",
        "elif (plain_sim_funct_name == 'jacc'):\n",
        "  plain_sim_funct = simcalc.q_gram_jacc_sim\n",
        "else:\n",
        "  raise Exception('This should not happen: sim function not allowed')\n",
        "\n",
        "#encoded\n",
        "if(encode_method == '2sh'):\n",
        "  if (encode_sim_funct_name == 'dice'):\n",
        "    encode_sim_funct = simcalc.q_gram_dice_sim\n",
        "  elif (encode_sim_funct_name == 'jacc'):\n",
        "    encode_sim_funct = simcalc.q_gram_jacc_sim\n",
        "  else:\n",
        "    raise Exception('This should not happen: hamming is not allowed to 2sh') # nao faz sentido\n",
        "else:\n",
        "  if (encode_sim_funct_name == 'dice'):\n",
        "    encode_sim_funct = simcalc.bit_array_dice_sim\n",
        "  elif (encode_sim_funct_name == 'hamm'):\n",
        "    encode_sim_funct = simcalc.bit_array_hamm_sim\n",
        "  elif (encode_sim_funct_name == 'jacc'):\n",
        "    encode_sim_funct = simcalc.bit_array_jacc_sim\n",
        "  else:\n",
        "    raise Exception('This should not happen: sim function not defined!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcn3LOlyKkLw"
      },
      "source": [
        "#### other variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SFJY6lrbbk9D"
      },
      "outputs": [],
      "source": [
        "if (encode_method == 'bf'):\n",
        "  encode_method_str = 'bf-%s-%s-%d-%s-%s' % (bf_hash_type, bf_num_hash_funct,\n",
        "                      bf_len, bf_encode, bf_harden)\n",
        "elif (encode_method == 'tmh'):\n",
        "  encode_method_str = 'tmh-%d-%s-%d-%d-%d' % (tmh_num_hash_bits,\n",
        "                      tmh_hash_funct, tmh_num_tables, tmh_key_len, tmh_val_len)\n",
        "elif (encode_method == '2sh'):\n",
        "  encode_method_str = '2sh-%d-%d-%d' % (cmh_max_rand_int,\n",
        "                                        cmh_num_hash_funct, cmh_num_hash_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hFWDlSwr7nwh"
      },
      "outputs": [],
      "source": [
        "today_str = time.strftime(\"%Y%m%d\", time.localtime())\n",
        "now_str =   time.strftime(\"%H%M\", time.localtime())\n",
        "today_time_str = time.strftime(\"%Y%m%d %H:%M:%S\", time.localtime())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nr4kNko7hrh"
      },
      "source": [
        "## Step 01: Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "54bL56R-QRtz"
      },
      "outputs": [],
      "source": [
        "#\n",
        "plain_rec_attr_val_dict, plain_attr_name_list, plain_num_rec_loaded, plain_soundex_val_dict =\\\n",
        "    load_data_set(lpath + 'datasets'+os.sep+'census'+os.sep+'a.csv',plain_attr_list,0,[2],\n",
        "                    header_line_flag=True,\n",
        "                    col_sep_char=';')\n",
        "#encoded\n",
        "encode_rec_attr_val_dict, encode_attr_name_list,encode_num_rec_loaded, encode_soundex_val_dict =\\\n",
        "    load_data_set(lpath +'datasets'+os.sep+'ncvr'+os.sep+'a.csv',encode_attr_list,0,[3],\n",
        "                    header_line_flag=True,\n",
        "                    col_sep_char=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DC0N8CTIkq2J"
      },
      "outputs": [],
      "source": [
        "#provisorio\n",
        "plain_attr_list = encode_attr_list\n",
        "plain_rec_attr_val_dict, plain_attr_name_list, plain_num_rec_loaded, plain_soundex_val_dict =\\\n",
        "    load_data_set(lpath + 'datasets'+os.sep+'ncvr'+os.sep+'b.csv',plain_attr_list,0,[3],\n",
        "                    header_line_flag=True,\n",
        "                    col_sep_char=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5SuOS9GSqNL",
        "outputId": "7d891204-ec4c-468c-ed30-6abb83d8479f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The two datasets have 170 overlapping records (10.0 %)\n"
          ]
        }
      ],
      "source": [
        "if (encode_attr_name_list != plain_attr_name_list):\n",
        "    print('*** Warning: Different attributes used in encoded and ' + \\\n",
        "          'plain-text files:')\n",
        "    print('***   Encoded file:   ', encode_attr_name_list)\n",
        "    print('***   Plain-text file:', plain_attr_name_list)\n",
        "# else:  # Set to same as encode\n",
        "#   plain_rec_attr_val_dict = encode_rec_attr_val_dict\n",
        "#   plain_attr_name_list =    encode_attr_name_list\n",
        "#   plain_num_rec_loaded =    encode_num_rec_loaded\n",
        "\n",
        "# Generate q-gram sets for records\n",
        "#\n",
        "encode_q_gram_dict = gen_q_gram_sets(encode_rec_attr_val_dict, q, padded_flag)\n",
        "plain_q_gram_dict =  gen_q_gram_sets(plain_rec_attr_val_dict, q, padded_flag)\n",
        "\n",
        "enc_id_set = set(encode_q_gram_dict.keys())\n",
        "plain_id_set = set(plain_q_gram_dict.keys())\n",
        "\n",
        "common_rec_id_set = enc_id_set.intersection(plain_id_set)\n",
        "\n",
        "print('The two datasets have %d overlapping records (%.1f %%)' % \\\n",
        "      (len(common_rec_id_set), 200*float(len(common_rec_id_set))/(len(enc_id_set)+len(plain_id_set)))\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GASQeDex7lwl"
      },
      "source": [
        "## Step 02\n",
        "\n",
        "If both graphs are available as pickle files then load and use them, otherwise encode the first data set according to the encoding settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uqJJ_G0oaB1D"
      },
      "outputs": [],
      "source": [
        "from graph_attack.utils.graph import SimGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wq2CbZW6aRXz"
      },
      "outputs": [],
      "source": [
        "# Initialise the two graphs\n",
        "# QG_sim_graph = SimGraph()  \n",
        "# BA_sim_graph = SimGraph()\n",
        "\n",
        "# Create the graph pickle file names, and if they both are available then\n",
        "# load the graphs from files\n",
        "#\n",
        "plain_attr_list_str =  str(plain_attr_list).replace(', ','_')[1:-1]\n",
        "encode_attr_list_str = str(encode_attr_list).replace(', ','_')[1:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oGLisEmM1BmE"
      },
      "outputs": [],
      "source": [
        "# The generated graph file names\n",
        "#          \n",
        "encode_str = 'encode-sim-graph-%s-%s-%d-%d-%s-%s-%s-%s-%.3f-%s' \\\n",
        "             % (encode_base_data_set_name, encode_attr_list_str,\n",
        "                encode_num_rec_loaded, q, str(padded_flag).lower(),\n",
        "                regre_model_str.lower(),\n",
        "                encode_sim_funct_name, encode_blck_method, min_sim,\n",
        "                encode_method_str)\n",
        "             \n",
        "plain_str = 'plain-sim-graph-%s-%s-%d-%d-%s-%s-%s-%s-%.3f-%s' % \\\n",
        "             (plain_base_data_set_name, plain_attr_list_str,\n",
        "              plain_num_rec_loaded, q, str(padded_flag).lower(),\n",
        "              regre_model_str.lower(),\n",
        "              plain_sim_funct_name, plain_blck_method, min_sim,\n",
        "              encode_method_str)\n",
        "\n",
        "encode_graph_file_name = encode_str + '.pickle'\n",
        "plain_graph_file_name = plain_str + '.pickle'\n",
        "\n",
        "graph_path = 'graphs/'\n",
        "\n",
        "encode_graph_file_name = graph_path + encode_graph_file_name\n",
        "plain_graph_file_name  = graph_path + plain_graph_file_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRgKvKurupzo"
      },
      "source": [
        "#### Step 02: encode dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qbtXrGxSuBwU"
      },
      "outputs": [],
      "source": [
        "from graph_attack.graph_attack import encode_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBc5OfxdJhfQ"
      },
      "source": [
        "encodando o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cNQAA2Yiuak2"
      },
      "outputs": [],
      "source": [
        "# encode_hash_dict,hashing_time,plain_num_ent,encode_num_ent,common_num_ent =\\\n",
        "# encode_hash_dict,hashing_time,plain_num_ent,encode_num_ent, =\\\n",
        "e = encode_ds(encode_q_gram_dict,encode_attr_list,encode_rec_attr_val_dict,\n",
        "                plain_rec_attr_val_dict,\n",
        "                encode_method,bf_len,q,bf_encode,\n",
        "                bf_hash_type=bf_hash_type,\n",
        "                bf_enc_param='clk',\n",
        "                bf_num_hash_funct=bf_num_hash_funct,\n",
        "                random_seed=101, # definir\n",
        "            )\n",
        "\n",
        "#TODO: rever esse metodo\n",
        "encode_hash_dict,hashing_time,plain_num_ent,encode_num_ent,common_num_ent = e[0],e[1],e[2],e[3],e[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVQHKl94uZNF"
      },
      "source": [
        "scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVq8-2A87nyt"
      },
      "source": [
        "## Step 03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0DyTqv_p7obN"
      },
      "outputs": [],
      "source": [
        "generated_graph_flag = True  # Graphs were generated in this run\n",
        "graph_path = lpath + 'saida' + os.sep\n",
        "reg_path = lpath + 'saida' + os.sep\n",
        "\n",
        "encode_graph_file_name = graph_path + 'encode_graph_file_name'\n",
        "plain_graph_file_name  = graph_path + 'plain_graph_file_name'\n",
        "bf_num_hash_funct,bf_hash_type\n",
        "# ['linear','isotonic','poly',]\n",
        "regre_model_str='linear'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QRhCiEzikq2L"
      },
      "outputs": [],
      "source": [
        "del genG\n",
        "from graph_attack.graph_attack import genG\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sjI4twqlkq2M"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "# logger.setLevel(logging.DEBUG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Ucli5HUQ3OU0"
      },
      "outputs": [],
      "source": [
        "plain_blck_method = 'none'\n",
        "plain_blck_method = 'minhash'\n",
        "regre_model_str='linear'\n",
        "\n",
        "# Get the overall minimum similarity for the graphs\n",
        "#\n",
        "min_sim = 1.0\n",
        "\n",
        "graph_sim_list = [('[0.2]',[0.2])] #('[0.9-0.5]',[0.9,0.7,0.5]),\n",
        "                                     #('[0.5]',[0.5])\n",
        "\n",
        "# Also get a sorted list of all similarities over all parameter settings\n",
        "#\n",
        "all_sim_set = set()\n",
        "\n",
        "for (sim_list_name, sim_list) in graph_sim_list:\n",
        "  min_sim = min(min_sim, min(sim_list))\n",
        "  all_sim_set = all_sim_set | set(sim_list)\n",
        "\n",
        "all_sim_list = sorted(all_sim_set)\n",
        "\n",
        "\n",
        "z = genG(plain_num_ent,encode_num_ent,\n",
        "        encode_method, # encoding\n",
        "        # encode\n",
        "        encode_sim_funct_name,\n",
        "        encode_hash_dict,\n",
        "        encode_q_gram_dict,\n",
        "        encode_rec_attr_val_dict,\n",
        "        encode_blck_method, #'hmlsh': Hamming LSH blocking 'minhash' :  Min-hash LSH blocking\n",
        "        #plain\n",
        "        plain_q_gram_dict,\n",
        "        plain_rec_attr_val_dict,\n",
        "        plain_sim_funct_name,\n",
        "        plain_blck_method,\n",
        "        #soundex\n",
        "        encode_soundex_val_dict,\n",
        "        plain_soundex_val_dict,\n",
        "        # encoded_plain_vars\n",
        "        # regression\n",
        "        regre_model_str,\n",
        "        plain_base_data_set_name,\n",
        "        plain_attr_list_str,\n",
        "        q,\n",
        "        padded_flag,\n",
        "        encode_method_str,\n",
        "        encode_base_data_set_name,plain_num_rec_loaded,encode_attr_list_str,encode_num_rec_loaded,\n",
        "        #outros\n",
        "        min_sim, # vem la de cima\n",
        "        all_sim_list,\n",
        "        #\n",
        "        num_samples = 20000, # regression num samples\n",
        "        random_seed=101,\n",
        "        ## anonimizacao\n",
        "        bf_hash_type=bf_hash_type,\n",
        "        bf_num_hash_funct=bf_num_hash_funct,\n",
        "        bf_len=bf_len,\n",
        "        bf_encode=bf_encode,\n",
        "        bf_harden='None',\n",
        "        # leitura dos grafos\n",
        "        graph_path = graph_path,\n",
        "        regre_file_path = reg_path,\n",
        "        plain_graph_file_name=plain_graph_file_name,\n",
        "        encode_graph_file_name=encode_graph_file_name,\n",
        "        # utilizar apenas atributos em comum\n",
        "        include_only_common = False,\n",
        "        common_rec_id_set={},#setar caso seja utilizado\n",
        "        #nao sei o que e isso mas estava hardoced\n",
        "        same_ba_blck = False,\n",
        "        #ajueste de similaridade\n",
        "        sim_diff_interval_size = 0.05,\n",
        "        sim_diff_adjust_flag=True) # True ainda nao funcionau\n",
        "\n",
        "QG_sim_graph, BA_sim_graph, plain_graph_num_node, encode_graph_num_node, attck_res_header_list, attck_res_val_list = z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDNh8_yCNyks"
      },
      "outputs": [],
      "source": [
        "# qg, ba = z\n",
        "# p = qg.sim_graph\n",
        "# g = ba.sim_graph #anonimizado\n",
        "# len(p.nodes),len(p.edges),'-',len(g.nodes),len(g.edges)\n",
        "# a = [(0.19230769230769232, 26), (0.2727272727272727, 22)]\n",
        "# list(zip(z,k))\n",
        "\n",
        "# import numpy as np\n",
        "# np.array(a) * 10\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# x_train, x_test, y_train, y_test = train_test_split(a, \n",
        "#                                                     [1,0], \n",
        "#                                                     test_size=0.25\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsWXx_oY7owK"
      },
      "source": [
        "## Step 04"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3hswyvuf7pWN"
      },
      "outputs": [],
      "source": [
        "from graph_attack.graph_attack import step04\n",
        "\n",
        "#variaveis importantes\n",
        "# qg_graph_node_id_dict = {}\n",
        "# ba_graph_node_id_dict = {}\n",
        "\n",
        "# qg_graph_node_id_dict = {}\n",
        "# ba_graph_node_id_dict = {}\n",
        "\n",
        "exp_params = [ plain_attr_list_str,plain_num_rec_loaded,encode_attr_list_str,\\\n",
        "    encode_num_rec_loaded,padded_flag,q,plain_sim_funct_name,\\\n",
        "    encode_sim_funct_name,encode_method_str,encode_blck_method,\\\n",
        "    plain_blck_method\n",
        "]\n",
        "# QG_sim_graph, BA_sim_graph, plain_graph_num_node, encode_graph_num_node, attck_res_header_list, attck_res_val_list\n",
        "\n",
        "### graph paramns\n",
        "\n",
        "# sim_hash_block_funct_list = ['all_in_one'] # 'all_in_one','hlsh'\n",
        "# sim_comp_funct_list = ['allpairs'] # 'allpairs', 'simtol'\n",
        "\n",
        "graph_sim_list = [('[0.2]',[0.2])]\n",
        "\n",
        "graph_node_min_degr_list = [('5',5)]\n",
        "\n",
        "sim_hash_num_bit_list = [('1000',1000)]\n",
        "sim_hash_match_sim = 0.9\n",
        "sim_hash_block_funct_list = ['hlsh']\n",
        "sim_comp_funct_list = ['simtol']\n",
        "\n",
        "graph_feat_list_list = [('all',['node_freq','max_sim','min_sim','degree',\n",
        "                            'degree_histo1','degree_histo2','sim_avr',\n",
        "                            'sim_std','egonet_degree','egonet_density',\n",
        "                            #'pagerank', # very time consuming\n",
        "                            'between_central',\n",
        "                            #'closeness_central', # very time consuming\n",
        "                            'degree_central',\n",
        "                            #'eigenvec_central', # very time consuming\n",
        "                            ]),\n",
        "                            \n",
        "                     ('no-histo',['node_freq','max_sim','min_sim','degree',\n",
        "                                   'sim_avr', 'sim_std','egonet_degree',\n",
        "                                   'egonet_density','between_central',\n",
        "                                   'degree_central', 'eigenvec_central',\n",
        "                            ]),\n",
        "                     \n",
        "                     ('one-central',['node_freq','max_sim','min_sim','degree',\n",
        "                            'degree_histo1', 'degree_histo2', 'sim_avr',\n",
        "                            'sim_std','egonet_degree', 'egonet_density',\n",
        "                            'degree_central',\n",
        "                            ]),\n",
        "                     \n",
        "                     ('no-nf-central',['max_sim','min_sim','degree',\n",
        "                                          'degree_histo1','degree_histo2',\n",
        "                                          'sim_avr','sim_std',\n",
        "                                          'egonet_degree','egonet_density']),\n",
        "                     \n",
        "                     ('no-central',['node_freq','max_sim','min_sim',\n",
        "                                   'degree','degree_histo1',\n",
        "                                   'degree_histo2','sim_avr','sim_std',\n",
        "                                   'egonet_degree','egonet_density']),\n",
        "                     \n",
        "                     ('node2vec', ['node2vec']),\n",
        "]\n",
        "\n",
        "graph_feat_select_list = [('all','all'),\n",
        "                          #('non-zero','nonzero'),\n",
        "                          #'top-10':10,\n",
        "                          #('top-10',10)\n",
        "                          #('min-std-0.4',0.4),\n",
        "                         ]\n",
        "\n",
        "# For the 'simtol' comparison approach set the similarity tolerance values:\n",
        "# lower: how much lower the encoded similarity can be,\n",
        "# upper: how much higher the encoded similarity can be\n",
        "#\n",
        "bf_hash_sim_low_tol =  0.01  # If the encoding are Bloom filters\n",
        "bf_hash_sim_up_tol =   0.25\n",
        "\n",
        "tmh_hash_sim_low_tol =  0.05  # If the encoding are tabulation min hashes\n",
        "tmh_hash_sim_up_tol =   0.05\n",
        "\n",
        "cmh_hash_sim_low_tol =  0.05  # If the encoding are tabulation min hashes\n",
        "cmh_hash_sim_up_tol =   0.01\n",
        "\n",
        "adj_hash_sim_low_tol =  0.05  # If the encoded similarities have been adjusted\n",
        "adj_hash_sim_up_tol =   0.05\n",
        "\n",
        "#ajustes\n",
        "anon_hash_sim_low_tol = bf_hash_sim_low_tol\n",
        "anon_hash_sim_up_tol = bf_hash_sim_up_tol\n",
        "\n",
        "\n",
        "hash_sim_min_tol = 0.05  # Also for the 'simtol' comparison approach, how much\n",
        "                         # tolerance to count the number of edges\n",
        "\n",
        "# Numpy and Scipy Cosine give the same results and are always best - so only\n",
        "# use those\n",
        "#\n",
        "#final_sim_funct_list = ['lsh_cosine','cosine_scipy','cosine_numpy',\n",
        "#                        'euclidean']\n",
        "#final_sim_funct_list = ['edge_sim_diff']\n",
        "\n",
        "# The parameters for the Hamming LSH blocking of the similarity hashes in the\n",
        "# graph matching step\n",
        "#\n",
        "# if('ncvoter' in encode_data_set_name or 'ncvr' in encode_data_set_name):\n",
        "graph_block_hlsh_num_sample =      20\n",
        "graph_block_hlsh_rel_sample_size = 10  # Divide bit array length by this\n",
        "\n",
        "# else:\n",
        "#   graph_block_hlsh_num_sample =      50\n",
        "#   graph_block_hlsh_rel_sample_size = 50  # Divide bit array length by this\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "c2sGMeUptmKo"
      },
      "outputs": [],
      "source": [
        "\n",
        "import shutil\n",
        "if os.name == 'nt':\n",
        "    try:\n",
        "        shutil.rmtree('feats')\n",
        "        os.remove('ataque.csv')\n",
        "        # os.remove('*.png')\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "# else:\n",
        "#     !rm -rf feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3AphHuC-0GGA"
      },
      "outputs": [],
      "source": [
        "# from scipy.optimize import linear_sum_assignment\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.ERROR)\n",
        "# logger.setLevel(logging.DEBUG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "v0-Lq0uwkq2R",
        "outputId": "0837906d-e0bc-4f5f-e122-1a2c899ce058"
      },
      "outputs": [],
      "source": [
        "del step04\n",
        "from graph_attack.graph_attack import step04\n",
        "step04(QG_sim_graph,BA_sim_graph,\n",
        "        graph_node_min_degr_list,\n",
        "        min_sim,\n",
        "        all_sim_list,\n",
        "        plain_rec_attr_val_dict,\n",
        "        encode_rec_attr_val_dict,\n",
        "        plain_q_gram_dict,\n",
        "        encode_q_gram_dict,\n",
        "        #\n",
        "        sim_comp_funct_list,\n",
        "        sim_hash_num_bit_list,\n",
        "        sim_hash_block_funct_list,\n",
        "        # sim_hash_match_sim,\n",
        "        #\n",
        "        hash_sim_min_tol,\n",
        "        adj_hash_sim_low_tol,\n",
        "        adj_hash_sim_up_tol,\n",
        "        anon_hash_sim_low_tol,\n",
        "        anon_hash_sim_up_tol,\n",
        "        #\n",
        "        plain_str,\n",
        "        encode_str,\n",
        "        #\n",
        "        graph_sim_list,\n",
        "        graph_feat_list_list,\n",
        "        graph_feat_select_list,\n",
        "        #\n",
        "        plain_base_data_set_name,\n",
        "        encode_base_data_set_name,\n",
        "        #\n",
        "        encode_method, \n",
        "        #\n",
        "        attck_res_header_list, attck_res_val_list,\n",
        "        #\n",
        "        sim_hash_match_sim=0, #all blocagem\n",
        "        # blocagem\n",
        "        graph_block_hlsh_rel_sample_size=graph_block_hlsh_rel_sample_size,\n",
        "        graph_block_hlsh_num_sample=graph_block_hlsh_num_sample,\n",
        "        #\n",
        "        plain_graph_num_node=0,\n",
        "        encode_graph_num_node=0,\n",
        "        attack_res_file_name='ataque.csv',\n",
        "        random_seed=101,\n",
        "        feat_path = 'feats' + os.sep,\n",
        "        sim_diff_adjust_flag=False,\n",
        "        exp_params=exp_params,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEk5jMa27pnH"
      },
      "source": [
        "## Step 05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhF-CNzQ7rHu"
      },
      "source": [
        "## Step 06"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SUmfNTtrkq2S",
        "outputId": "cc0ab6e0-750a-4324-cd30-bc5c81e81896"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYqdwK3pkq2S"
      },
      "source": [
        "## Rascunho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCzobUr5mlOA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOFrW0f_kq2N"
      },
      "source": [
        "#### Debug populando grafos manualmente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxdVa7M_kq2N"
      },
      "outputs": [],
      "source": [
        "#plain\n",
        "QG_sim_graph = SimGraph()\n",
        "plain_node_val_dict = {}\n",
        "\n",
        "#encoded\n",
        "BA_sim_graph = SimGraph()\n",
        "encode_node_val_dict = {}\n",
        "encode_key_q_gram_key_dict = {}\n",
        "ba_graph_node_id_dict = {}\n",
        "encode_plain_node_val_dict = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWdq1xQ-kq2N"
      },
      "source": [
        "nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "382BosSDkq2N"
      },
      "outputs": [],
      "source": [
        "for (ent_id, q_gram_set) in plain_q_gram_dict.items():\n",
        "    node_key_val = tuple(sorted(q_gram_set))  # Sets cannot be dictionary keys\n",
        "    plain_node_val_dict[node_key_val] = q_gram_set\n",
        "    # Only keep none empty attribute values\n",
        "    #\n",
        "    QG_sim_graph.add_rec(node_key_val, ent_id,filter(None, plain_rec_attr_val_dict[ent_id]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_QkWSwckq2N"
      },
      "outputs": [],
      "source": [
        "for (ent_id, bit_array) in encode_hash_dict.items():\n",
        "    q_gram_set = encode_q_gram_dict[ent_id]\n",
        "    #apenas para BFs\n",
        "    node_key_val = str(bit_array)  # Bit arrays cannot be dictionary keys\n",
        "    \n",
        "    encode_node_val_dict[node_key_val] = (q_gram_set, bit_array)\n",
        "    \n",
        "    if(node_key_val in encode_key_q_gram_key_dict):\n",
        "        assert encode_key_q_gram_key_dict[node_key_val] == tuple(sorted(q_gram_set))\n",
        "    else:\n",
        "        encode_key_q_gram_key_dict[node_key_val] = tuple(sorted(q_gram_set))\n",
        "\n",
        "    # Only keep none empty attribute values\n",
        "    #\n",
        "    BA_sim_graph.add_rec(node_key_val, ent_id,\n",
        "                        filter(None, encode_rec_attr_val_dict[ent_id]))\n",
        "        \n",
        "    if(ent_id not in ba_graph_node_id_dict):\n",
        "        ba_graph_node_id_dict[ent_id] = node_key_val\n",
        "        plain_node_key_val = tuple(sorted(q_gram_set))\n",
        "        encode_plain_node_val_dict[plain_node_key_val] = bit_array\n",
        "    # fim do for 02 (encode_hash_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Do0wVutkq2N"
      },
      "source": [
        "edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm4OWZhikq2O"
      },
      "outputs": [],
      "source": [
        "from graph_attack.utils.indexing import MinHashLSH\n",
        "\n",
        "plain_sample_size = 4\n",
        "plain_num_samples = 50\n",
        "QG_min_hash = MinHashLSH(plain_sample_size, plain_sample_size, 101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KDg_A00kq2O"
      },
      "outputs": [],
      "source": [
        "#mover isso para proximo da blocagem\n",
        "#isso tem que vir antes de remover os singletons\n",
        "# start_time = time.time()\n",
        "plain_blck_method='minhash'\n",
        "\n",
        "qg_blck_dict = {}\n",
        "if(plain_blck_method == 'minhash'):\n",
        "    for (node_key_val, q_gram_set) in plain_node_val_dict.items():\n",
        "        qg_band_hash_sig_list = QG_min_hash.hash_q_gram_set(q_gram_set)\n",
        "    \n",
        "        for min_hash_val in qg_band_hash_sig_list:\n",
        "            min_hash_val_tuple = tuple(min_hash_val)\n",
        "            min_hash_key_val_set = qg_blck_dict.get(min_hash_val_tuple, set())\n",
        "            min_hash_key_val_set.add(node_key_val)\n",
        "            qg_blck_dict[min_hash_val_tuple] = min_hash_key_val_set\n",
        "\n",
        "num_rec_pair = 0\n",
        "min_hash_block_size_list = []\n",
        "for min_hash_key_val_set in qg_blck_dict.values():\n",
        "    block_size = len(min_hash_key_val_set)\n",
        "    min_hash_block_size_list.append(block_size)\n",
        "    num_rec_pair += block_size*(block_size-1) / 2\n",
        "\n",
        "num_rec_pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnEI8nqvkq2O"
      },
      "outputs": [],
      "source": [
        "for (bn, min_hash_key_val_set) in enumerate(qg_blck_dict.values()):\n",
        "        if ((bn > 0) and (bn % 10000 == 0)):\n",
        "            logging.debug('    Processed %d of %d blocks' % (bn, num_blocks))\n",
        "        \n",
        "        if (len(min_hash_key_val_set) > 1):\n",
        "            min_hash_key_val_list = sorted(min_hash_key_val_set)\n",
        "\n",
        "            for (i, node_key_val1) in enumerate(min_hash_key_val_list[:-1]):\n",
        "                q_gram_set1 = plain_node_val_dict[node_key_val1]\n",
        "                assert q_gram_set1 == set(node_key_val1)\n",
        "                \n",
        "                for node_key_val2 in min_hash_key_val_list[i+1:]:\n",
        "                    q_gram_set2 = plain_node_val_dict[node_key_val2]\n",
        "                    assert q_gram_set2 == set(node_key_val2)\n",
        "                    plain_pair_sim = plain_sim_funct(q_gram_set1, q_gram_set2)\n",
        "                    # If needed adjust the edge similarity - - - - - - - - - - - - - - -\n",
        "                    #\n",
        "                    if (sim_diff_adjust_flag == True):\n",
        "                        if(encode_method == 'bf'):\n",
        "                            full_q_gram_set = q_gram_set1.union(q_gram_set2)\n",
        "                            input_array = [[plain_pair_sim, len(full_q_gram_set)]]\n",
        "                        if(encode_method in ['tmh', '2sh']):\n",
        "                            input_array = [[plain_pair_sim]]\n",
        "                        \n",
        "                        if(regre_model_str == 'linear'):\n",
        "                            plain_pair_sim = regre_model.predict(input_array)[0][0]\n",
        "                        elif(regre_model_str == 'poly'):\n",
        "                            poly = PolynomialFeatures(degree=2)\n",
        "                            ori_sim_val = poly.fit_transform(input_array)\n",
        "                            plain_pair_sim = regre_model.predict(ori_sim_val)[0][0]\n",
        "                        else: # isotonic\n",
        "                            plain_pair_sim = regre_model.predict(plain_pair_sim)[0]\n",
        "\n",
        "                    if (plain_pair_sim >= min_sim):\n",
        "                        QG_sim_graph.add_edge_sim(node_key_val1, node_key_val2,plain_pair_sim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26sTGmfwkq2O"
      },
      "source": [
        "edges anon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlWXVuv1kq2O"
      },
      "outputs": [],
      "source": [
        "ba_blck_dict = {}\n",
        "\n",
        "enc_sample_size = plain_sample_size\n",
        "enc_num_samples = plain_num_samples\n",
        "\n",
        "for (node_key_val, q_gram_set_bit_array) in encode_node_val_dict.items():\n",
        "    q_gram_set, bit_array = q_gram_set_bit_array\n",
        "    ba_band_hash_sig_list = QG_min_hash.hash_q_gram_set(q_gram_set)\n",
        "    for min_hash_val in ba_band_hash_sig_list:\n",
        "        min_hash_val_tuple = tuple(min_hash_val)\n",
        "        min_hash_key_val_set = ba_blck_dict.get(min_hash_val_tuple, set())\n",
        "        min_hash_key_val_set.add(node_key_val)\n",
        "        ba_blck_dict[min_hash_val_tuple] = min_hash_key_val_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPDcYqEGkq2O"
      },
      "outputs": [],
      "source": [
        "ba_sim_list        = []\n",
        "qg_sim_list        = []\n",
        "qg_num_q_gram_list = []\n",
        "sim_abs_diff_list  = []\n",
        "sim_sample_dict = {'0-1': 0, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 0, '5-6': 0, \n",
        "                    '6-7': 0, '7-8': 0, '8-9': 0, '9-10': 0}\n",
        "\n",
        "for (bn, min_hash_key_val_set) in enumerate(ba_blck_dict.values()):\n",
        "    if (len(min_hash_key_val_set) > 1):\n",
        "        min_hash_key_val_list = sorted(min_hash_key_val_set)\n",
        "        for (i, node_key_val1) in enumerate(min_hash_key_val_list[:-1]):\n",
        "            q_gram_set1, bit_array1 = encode_node_val_dict[node_key_val1]\n",
        "\n",
        "            for node_key_val2 in min_hash_key_val_list[i+1:]:\n",
        "                q_gram_set2, bit_array2 = encode_node_val_dict[node_key_val2]\n",
        "                plain_pair_sim = plain_sim_funct(q_gram_set1, q_gram_set2)\n",
        "                encode_pair_sim = encode_sim_funct(bit_array1, bit_array2)\n",
        "                if(sim_diff_adjust_flag == True):\n",
        "                    # sim_class = check_sim_class(plain_pair_sim)\n",
        "                    sim_class = 'None'\n",
        "                    if(sim_class != 'None'):\n",
        "                        if(sim_sample_dict[sim_class] < 500):\n",
        "                            full_q_set = q_gram_set1.union(q_gram_set2)\n",
        "                            ba_sim_list.append(encode_pair_sim)\n",
        "                            qg_sim_list.append(plain_pair_sim)\n",
        "                            qg_num_q_gram_list.append(len(full_q_set))\n",
        "                            sim_sample_dict[sim_class]+=1\n",
        "                            \n",
        "                            abs_sim_diff = abs(encode_pair_sim - plain_pair_sim)\n",
        "                            sim_abs_diff_list.append(abs_sim_diff)\n",
        "                #fim do for adjust_flag\n",
        "                enc_min_sim = min_sim\n",
        "                if (encode_pair_sim >= min_sim):\n",
        "                    BA_sim_graph.add_edge_sim(node_key_val1, node_key_val2,encode_pair_sim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ8JvsP3kq2P"
      },
      "source": [
        "Generate a list of similarities of all edges between records from the same entities (not just q-gram or bit array values) and count the number of such edges not in the intersection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5WdL_oWkq2P"
      },
      "outputs": [],
      "source": [
        "plain_sim_graph =  QG_sim_graph.sim_graph  # Short cuts\n",
        "encode_sim_graph = BA_sim_graph.sim_graph\n",
        "\n",
        "qg_edge_dict = {}\n",
        "ba_edge_dict = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYv2eDyfkq2P"
      },
      "outputs": [],
      "source": [
        "#grafo plano\n",
        "for (node_key_val1, node_key_val2) in plain_sim_graph.edges():\n",
        "    ent_id_set1 = plain_sim_graph.node[node_key_val1]['ent_id_set']\n",
        "    ent_id_set2 = plain_sim_graph.node[node_key_val2]['ent_id_set']\n",
        "    edge_sim =    plain_sim_graph.edges[node_key_val1,node_key_val2]['sim']\n",
        "\n",
        "    for ent_id1 in ent_id_set1:\n",
        "        for ent_id2 in ent_id_set2:\n",
        "            ent_id_pair = tuple(sorted([ent_id1,ent_id2]))\n",
        "            assert ent_id_pair not in qg_edge_dict, ent_id_pair\n",
        "            qg_edge_dict[ent_id_pair] = edge_sim\n",
        "\n",
        "#grafo codificado\n",
        "for (node_key_val1, node_key_val2) in encode_sim_graph.edges():\n",
        "    ent_id_set1 = encode_sim_graph.node[node_key_val1]['ent_id_set']\n",
        "    ent_id_set2 = encode_sim_graph.node[node_key_val2]['ent_id_set']\n",
        "    edge_sim =    encode_sim_graph.edges[node_key_val1,node_key_val2]['sim']\n",
        "\n",
        "    for ent_id1 in ent_id_set1:\n",
        "        for ent_id2 in ent_id_set2:\n",
        "            ent_id_pair = tuple(sorted([ent_id1,ent_id2]))\n",
        "            assert ent_id_pair not in ba_edge_dict, ent_id_pair\n",
        "            ba_edge_dict[ent_id_pair] = edge_sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBXXc0lFkq2P"
      },
      "outputs": [],
      "source": [
        "# qg_edge_dict.keys()\n",
        "# plain_sim_graph.edges\n",
        "# encode_sim_graph.edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9LXOm2Ykq2P"
      },
      "outputs": [],
      "source": [
        "common_edge_set =       set(qg_edge_dict.keys()) & set(ba_edge_dict.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goJF1etskq2P"
      },
      "source": [
        "##### continuação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2paJPfQkq2Q"
      },
      "outputs": [],
      "source": [
        "list(QG_sim_graph.sim_graph.nodes)[0],list(BA_sim_graph.sim_graph.nodes)[0]\n",
        "# encode_key_q_gram_key_dict\n",
        "import networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwvK0aiKkq2Q"
      },
      "outputs": [],
      "source": [
        "ba_graph_num_edges = networkx.number_of_edges(BA_sim_graph.sim_graph)\n",
        "ba_graph_num_singleton = BA_sim_graph.remove_singletons()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8obq1Jskq2Q"
      },
      "outputs": [],
      "source": [
        "BA_sim_graph.remove_singletons()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyUQk9lQ6-MM"
      },
      "source": [
        "#### Step 03: Ler arquivos com os grafos (acho que nao esta no visio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQsxm23l687j"
      },
      "outputs": [],
      "source": [
        "# Two dictionaries that can be usedt to get similar nodes in both graphs\n",
        "# since graph node values cannot be compared.\n",
        "#\n",
        "qg_graph_node_id_dict = {}\n",
        "ba_graph_node_id_dict = {}\n",
        "\n",
        "if (os.path.isfile(plain_graph_file_name) and \\\n",
        "    os.path.isfile(encode_graph_file_name)):\n",
        "  print('Load graphs from pickle files:')\n",
        "  print('  Plain-text graph file:', plain_graph_file_name)\n",
        "  print('  Encoded graph file:   ', encode_graph_file_name)\n",
        "  print()\n",
        "\n",
        "  generated_graph_flag = False  # Graphs were not generated in this run\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  QG_sim_graph.sim_graph = networkx.read_gpickle(plain_graph_file_name)\n",
        "  BA_sim_graph.sim_graph = networkx.read_gpickle(encode_graph_file_name)\n",
        "\n",
        "  print('  Time for loading the q-gram and bit-array similarity graphs: ' + \\\n",
        "        '%.2f secs' % (time.time() - start_time)\n",
        "  )\n",
        "  # print(' ', auxiliary.get_memory_usage())\n",
        "  print()\n",
        "    \n",
        "  for node_key_val in QG_sim_graph.sim_graph.nodes():\n",
        "    qg_id_set = QG_sim_graph.sim_graph.node[node_key_val]['ent_id_set']\n",
        "    \n",
        "    for qg_id in qg_id_set:\n",
        "      qg_graph_node_id_dict[qg_id] = node_key_val\n",
        "      \n",
        "  for node_key_val in BA_sim_graph.sim_graph.nodes():\n",
        "    ba_id_set = BA_sim_graph.sim_graph.node[node_key_val]['ent_id_set']\n",
        "    \n",
        "    for ba_id in ba_id_set:\n",
        "      ba_graph_node_id_dict[ba_id] = node_key_val"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2SctUpXT-Bb5",
        "1nr4kNko7hrh",
        "GASQeDex7lwl",
        "HRgKvKurupzo"
      ],
      "name": "GraphAttack.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "979abc0a35086451dcf41b2c584779315bebbe9a84e446060e411448c894138c"
    },
    "kernelspec": {
      "display_name": "tl_pprl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
